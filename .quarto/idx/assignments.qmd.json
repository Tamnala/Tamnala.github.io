{"title":"Assignments","markdown":{"yaml":{"title":"Assignments","editor":"visual"},"headingText":"Assignment #1","containsRefs":false,"markdown":"\n\n\n**ANALYZE SURVEY**\n\n**a-c**: The questions on the survey are relatively simple and non complex. Given that they relate to movie rentals they are particular to this topic. They begin with multiple choice/ranking questions and then are set up with multiple choice questions, but for the most part questions that are strictly qualitative.\n\nPersonally, my only criticism involves design. The questions seem long and from a survey design perspective not intuitive for mobile users.\n\n**d,3-7**: The survey is fairly intuitive and does not need editing, it does not deviate much from pre-existing surveys. Adding the UTD logo personalizes it to the university which makes it useful for UTD users.\n\n**FURTHER ASSIGNMENT**\n\n**1-4**: Inserting this block questions allows for the introduction of multiple questions at a time, assuming these blocks are preexisting they create an easier survey experience.\n\n# Assignment #2\n\nThe difference between the two programs comes down to intuition. If the purpose is to visualize data from a non social science research perspective GTrends is much better at doing this. However, if the goal is to both analyze and perform research the gtrends package is much better.\n\nIt would be interesting to perform an analysis importing the csv from google versus using the gtrends package.\n\n# Assignment #3\n\n**3a.**: The Biden-Xi analysis allows for an understanding of what is going on within United States international relations and what revolves around the conversation. Terms like **coronavirus** and **fentanyl** stand out, because of the controversy surrounding COVID-19, as well as the flow of fentanyl into the United States that is in part due to the drug coming from the souther border being of Chinese origin. In addition to the aforementioned terms it is important to note the role of human rights and the tension between rising Chinese hegemony and the United States hegemonic position being challenged by it. Since the U.S. represents Western Liberal democratic order it is possible to see how **human rights** ties into the crises involving both Tibetans and Uyghers in China.\n\n![](images/3.1.png)\n\n**3b.**: Looking over time, it is important to note that at least within the plot there is very little change in the use of the term \"american\" and this is very similar to the the use of the word \"people\". Across time, these terms do not look like they change across time. There are however a couple of anomalies within the plot - The Trump 2017 speach being one of these instances, where his use of the term \"american\" is greater than everybody else. An explanation for this may be due to his populist platform that appeases to his voter base. A similar thing is found with people in both Johnson and Nixon regarding people, but a qualitative analysis may be necessary to know why they used these terms to the extent they did. For analyses regarding ideology it would be interesting to do a similar analysis between the term \"communist\" and \"socialist\" - I attempted an analysis and was not able to get a plot for it.\n\n![](images/3.2.png)\n\n# Assignment #5\n\n1.  There were issues in my ability to access this data. However, listed below is what it looked like. As you can tell there is an original ID and secret. Issues regarding third-party apps made it difficult to continue to the data analysis portion of the assignment.\n\n```         \n\\# = Autenthication = \\# \\# \\# yt_oauth(\"207302960577-7uqd83qilb3586f0dj77p2h4hq06kjs0.apps.googleusercontent.com\",\"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\", token = \"\")\n\nyt_israelprotest = yt_search(term = \"Israel protest\")\n\n# List of categories (region filter: US)\n\nvideocat_us= list_videocats(c(region_code = \"us\")) \\# = Download and prepare data = \\# mostpop = list_videos()\n\nmostpop_us = list_videos(video_category_id = \"25\", region_code = \"US\", max_results = 10)\n\n# Find the channel ID in the source page\n\n# Alternatively, from get_video_details\n\n# = Channel stats =\n\nnbcnews_stat = get_channel_stats(\"UCeY0bbntWzzVIaj2z3QigXg\") nbcnews_detail = get_video_details(video_id = \"to0YqKKRIWY\")\n\n# = Videos =\n\ncurl::curl_version() httr::set_config(httr::config(http_version = 0)) \\# Fix curl issue\n\nnbc_videos1 = yt_search(term=\"\", type=\"video\", channel_id = \"UCeY0bbntWzzVIaj2z3QigXg\") nbc_videos = nbc_videos1 %\\>% mutate(date = as.Date(publishedAt)) %\\>% filter(date \\> \"2022-11-27\") %\\>% arrange(date) samplecomment = get_comment_threads(c(video_id = \"to0YqKKRIWY\"), max_results = 600) samplecomment2 = get_all_comments(c(video_id = \"to0YqKKRIWY\"), max_results = 600) \\# = Comments, may take a long time \\# nbc_comments = lapply(as.character(nbc_videos1\\$video_id), function(x){ get_comment_threads(c(video_id = x), max_results = 101) }) \n```\n\na\\. An analysis of CNN would be the following. Keeping all things the same, except the channel ID.\n\n```         \n# = Autenthication = #\n# \n# \nyt_oauth(\"207302960577-7uqd83qilb3586f0dj77p2h4hq06kjs0.apps.googleusercontent.com\",\"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\", token = \"\")\n\nyt_israelprotest = yt_search(term = \"Israel protest\")\n\n# List of categories (region filter: US)\nvideocat_us= list_videocats(c(region_code = \"us\"))\n# = Download and prepare data = #\nmostpop = list_videos()\n\nmostpop_us = list_videos(video_category_id = \"25\",   region_code = \"US\", max_results = 10)\n\n\n# Find the channel ID in the source page\n# Alternatively, from get_video_details\n# = Channel stats = #\n\ncnn_stat = get_channel_stats(\"UCeY0bbntWzzVIaj2z3QigXg\")\ncnn_detail = get_video_details(video_id = \"to0YqKKRIWY\")\n\n\n\n# = Videos = #\ncurl::curl_version()\nhttr::set_config(httr::config(http_version = 0)) # Fix curl issue\n\ncnn_videos1 = yt_search(term=\"\", type=\"video\", channel_id = \"UCeY0bbntWzzVIaj2z3QigXg\")\ncnn_videos = nbc_videos1 %>%\n  mutate(date = as.Date(publishedAt)) %>%\n  filter(date > \"2022-11-27\") %>%\n  arrange(date)\nsamplecomment = get_comment_threads(c(video_id = \"to0YqKKRIWY\"), max_results = 600)\nsamplecomment2 = get_all_comments(c(video_id = \"to0YqKKRIWY\"), max_results = 600)\n# = Comments, may take a long time #\ncnn_comments = lapply(as.character(nbc_videos1$video_id), function(x){\n  get_comment_threads(c(video_id = x), max_results = 101)\n})\n```\n\n3.  I can potentially use quanteda, but given the difficulty accessing third party apps it does not seem possible. It would be interesting to see a data scientist analyzing their own child do this.\n\n# Assignment #6\n\n1.  These are the wordclouds from the rscript textmining and the code used\n\n```         \n \\# Data Method: Text mining \\# File: textmining1.R \\# Theme: Download text data from web and create wordcloud\n\n    # Install the easypackages package\n\n    install.packages(\"easypackages\") library(easypackages)\n\n    # Download text data from website\n\n    mlkLocation \\<-URLencode(\"http://www.analytictech.com/mb021/mlk.htm\")\n\n    # use htmlTreeParse function to read and parse paragraphs\n\n    doc.html\\<- htmlTreeParse(mlkLocation, useInternal=TRUE) mlk \\<- unlist(xpathApply(doc.html, '//p', xmlValue)) mlk head(mlk, 3)\n\n    # Vectorize mlk\n\n    words.vec \\<- VectorSource(mlk)\n\n    # Check the class of words.vec\n\n    class(words.vec)\n\n    # Create Corpus object for preprocessing\n\n    words.corpus \\<- Corpus(words.vec) inspect(words.corpus)\n\n    # Turn all words to lower case\n\n    words.corpus \\<- tm_map(words.corpus, content_transformer(tolower))\n\n    # Remove punctuations, numbers\n\n    words.corpus \\<- tm_map(words.corpus, removePunctuation) words.corpus \\<- tm_map(words.corpus, removeNumbers)\n\n    # How about stopwords, then uniform bag of words created\n\n    words.corpus \\<- tm_map(words.corpus, removeWords, stopwords(\"english\"))\n\n    # Create Term Document Matrix\n\n    tdm \\<- TermDocumentMatrix(words.corpus) inspect(tdm)\n\n    m \\<- as.matrix(tdm) wordCounts \\<- rowSums(m) wordCounts \\<- sort(wordCounts, decreasing=TRUE) head(wordCounts)\n\n    # Create Wordcloud\n\n    cloudFrame\\<-data.frame(word=names(wordCounts),freq=wordCounts)\n\n    set.seed(1234) wordcloud(cloudFrame$word,cloudFrame$freq) wordcloud(names(wordCounts),wordCounts, min.freq=3,random.order=FALSE, max.words=500,scale=c(3,.5), rot.per=0.35,colors=brewer.pal(8,\"Dark2\"))\n    \n```\n\n![](images/a6.2.png)\n\n![](images/a6.1.png)\n\nThe following is from the Winston Churchill speech, code and images\n\n```         \n \\# Download text data from website wcLocation \\<-URLencode(\"http://www.historyplace.com/speeches/churchill-hour.htm\")\n\n# use htmlTreeParse function to read and parse paragraphs\n\ndoc.html\\<- htmlTreeParse(wcLocation, useInternal=TRUE) wc \\<- unlist(xpathApply(doc.html, '//p', xmlValue)) wc head(wc, 3)\n\n# Vectorize wc\n\nwords.vec \\<- VectorSource(wc)\n\n# Check the class of words.vec\n\nclass(words.vec)\n\n# Create Corpus object for preprocessing\n\nwords.corpus \\<- Corpus(words.vec) inspect(words.corpus)\n\n# Turn all words to lower case\n\nwords.corpus \\<- tm_map(words.corpus, content_transformer(tolower))\n\n# Remove punctuations, numbers\n\nwords.corpus \\<- tm_map(words.corpus, removePunctuation) words.corpus \\<- tm_map(words.corpus, removeNumbers)\n\n# How about stopwords, then uniform bag of words created\n\nwords.corpus \\<- tm_map(words.corpus, removeWords, stopwords(\"english\"))\n\n# Create Term Document Matrix\n\ntdm \\<- TermDocumentMatrix(words.corpus) inspect(tdm)\n\nm \\<- as.matrix(tdm) wordCounts \\<- rowSums(m) wordCounts \\<- sort(wordCounts, decreasing=TRUE) head(wordCounts)\n\n# Create Wordcloud\n\ncloudFrame\\<-data.frame(word=names(wordCounts),freq=wordCounts)\n\nset.seed(1234) wordcloud(cloudFrame$word,cloudFrame$freq) wordcloud(names(wordCounts),wordCounts, min.freq=3,random.order=FALSE, max.words=500,scale=c(3,.5), rot.per=0.35,colors=brewer.pal(8,\"Dark2\")) \\~\\~\\~\n```\n\n![](images/a6.3.png)\n\n![](images/a6.4.png)\n\nCode for running rvest01\n\n```         \n## Workshop: Scraping webpages with R rvest package \\# Prerequisites: Chrome browser, Selector Gadget\n\n#install.packages(\"tidyverse\") library(tidyverse) #install.packages(\"rvest\") library(rvest)\n\nurl \\<- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves' #Reading the HTML code from the Wiki website wikiforreserve \\<- read_html(url) class(wikiforreserve)\n\n## Get the XPath data using Inspect element feature in Safari, Chrome or Firefox\n\n\\## At Inspect tab, look for\n\n<table class=....>\n\ntag. Leave the table close \\## Right click the table and Copy XPath, paste at html_nodes(xpath =)\n\nforeignreserve \\<- wikiforreserve %\\>% html_nodes(xpath='//\\*[@id=\"mw-content-text\"]/div/table\\[1\\]') %\\>% html_table() class(foreignreserve) fores = foreignreserve\\[\\[1\\]\\]\n\nnames(fores) \\<- c(\"Rank\", \"Country\", \"Forexres\", \"Date\", \"Change\", \"Sources\") colnames(fores)\n\nhead(fores\\$Country, n=10)\n\n## Clean up variables\n\n## What type is Rank?\n\n## How about Date?\n\n# Remove trailing notes in Date variable\n\nlibrary(stringr) fores$newdate = str_split_fixed(fores$Date, \"\\\\\\[\", n = 2)\\[, 1\\]\n\nwrite.csv(fores, \"fores.csv\", row.names = FALSE) \\~\\~\\~\n```\n\nImage of csv \"Fores\"\n\n![](images/a6.5.png)\n\nScript rvest02\n\n```         \n## Workshop: Scraping webpages with R rvest package \\# Prerequisites: Chrome browser, Selector Gadget\n\n# install.packages(\"tidyverse\")\n\nlibrary(tidyverse) \\# install.packages(\"rvest\") url1 = \"https://www.imdb.com/search/title/?release_date=2022-01-01,2023-01-01\" imdb2022 \\<- read_html(url1) rank_data_html \\<- html_nodes(imdb2022,'.text-primary') rank_data \\<- as.numeric(html_text(rank_data_html)) head(rank_data, n = 10) title_data_html \\<- html_nodes(imdb2022,'.lister-item-header a') title_data \\<- html_text(title_data_html)\n\nhead(title_data, n =20)\n```\n\nThe difficulty of this is that I am unsure as to how to make this visual.\n\n# Assignment #7\n\n1.  Code for first part of govdata with output\n\n```         \n    ## Scraping Government data\n    ## Website: GovInfo (https://www.govinfo.gov/app/search/)\n    ## Prerequisite: Download from website the list of files to be downloaded\n    ## Designed for background job\n\n    # Start with a clean plate and lean loading to save memory\n     \n    gc(reset=T)\n    rm(list = ls())\n\n    #install.packages(c(\"purrr\", \"magrittr\")\n    install.packages(\"purr\")\n    install.packages(\"magrittr\")\n    library(purrr)\n    library(magrittr)\n\n    ## Set path for reading the listing and home directory\n    ## For Windows, use \"c:\\\\directory\\\\subdirectory\"\n    library(readr)\n\n\n    govfiles= read_csv(\"govinfo-search-results-2023-12-05T16_37_50.csv\")\n    View(govfiles)\n\n\n    # Directory to save the pdf's\n    save_dir <- \"C:\\\\epps6302\\\\pdf\"\n\n    # Function to download pdfs\n    download_govfiles_pdf <- function(url, id) {\n      tryCatch({\n        destfile <- paste0(save_dir, \"govfiles_\", id, \".pdf\")\n        download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n        Sys.sleep(runif(1, 1, 3))  # Important: random sleep between 1 and 3 seconds to avoid suspicion of \"hacking\" the server\n        return(paste(\"Successfully downloaded:\", url))\n      },\n      error = function(e) {\n        return(paste(\"Failed to download:\", url))\n      })\n    }\n\n    # Download files, potentially in parallel for speed\n    # Simple timer, can use package like tictoc\n    start.time <- Sys.time()\n    message(\"Starting downloads\")\n    results <- 1:length(pdf_govfiles_url) %>% \n      purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\n    message(\"Finished downloads\")\n    end.time <- Sys.time()\n    time.taken <- end.time - start.time\n    time.taken\n\n    # Print results\n    print(results)\n```\n\n![](images/a7.1.png)\n\n118th Congress Congressional Hearings in Committee on Foreign Affairs csv\n\n![](images/a7.2.png)\n\nA main issue with the parallel R file is the issue of cores and difficulty with overcoming this and constantly receiving and error. Code Below and output as well\n\n```         \n## Parallelization\n## Prerequisite: Multiple core on CPU\n\n# Load the parallel package\nlibrary(parallel)\nlibrary(pdftools)\n\n# Create a function to be applied in parallelizing later jobs\nread_pdf_to_text <- function(uri) {\n  text <- pdftools::pdf_text(uri)\n  return(text)\n}\n# For mac\n\npdf_texts <- mclapply(pdfpath, read_pdf_to_text, mc.cores = num_cores)\ntoc()\n\n## For all platforms\n# Load the parallel package\nlibrary(parallel)\nlibrary(pdftools)\n# Get the number of cores available on your machine\nnum_cores <- detectCores()\n\n# Initialize a cluster with the number of available cores\ncl <- makeCluster(num_cores)\n\n# Load libraries and functions in each cluster\nclusterEvalQ(cl, library(pdftools))\n\n# Define the function to be parallelized after making the clusters\nread_pdf_to_text <- function(uri) {\n  text <- pdftools::pdf_text(uri)\n  return(text)\n}\n\n# Export any libraries or objects that will be used within the parallel code\n\n# Perform the parallel computation\ntic()\npdf_texts <- parLapply(cfa_ch, read_pdf_to_text, mc.cores = num_cores)\ntoc()\n\n# Don't forget to stop the cluster\nstopCluster(cl)\n```\n\n![](images/a7.3.png)\n\n# Assignment #8\n\n1.  A couple of observations regarding setting up the census data key. First, it is a relative simple process. However, the second, it is much faster if one does not use an .edu email.\n\nMy key is: 311dcd5892541c0eaa28747e5024f1e308c5xxxx\n\n2.  Below are 2019 and 2009 codes and images. Although the assignment asks for 2020, this is not available.\n\n```         \n\n# Get a list of American Community Survey (ACS) 2019 variables\nacs19 = tidycensus::load_variables(2019, \"acs5\", cache = TRUE)\nacs19_Profile = load_variables(2019 , \"acs5/profile\", cache = TRUE)\nus_median_age19 <- get_acs(\n  geography = \"state\",\n  variables = \"B01002_001\",\n  year = 2019,\n  survey = \"acs1\",\n  geometry = TRUE,\n  resolution = \"20m\"\n) %>%\n  shift_geometry()\n\nplot(us_median_age19$geometry)\nggplot(data = us_median_age19, aes(fill = estimate)) + \n  geom_sf(col=\"white\") +  # Why color is white?\n  theme_bw() +\n  scale_fill_distiller(palette = \"PuBuGn\",  # Try other palette?\n                       direction = 1) + \n  labs(title = \"  Median Age by State, 2019\",\n       caption = \"Data source: 2019 1-year ACS, US Census Bureau\",\n       fill = \"\", family=\"Palatino\") +\n  theme(legend.position=c(.08,.6), legend.direction=\"vertical\") +\n  theme(text = element_text(family = \"Palatino\"), plot.title = element_text(hjust = 0.5))\n\n# Get a list of American Community Survey (ACS) 2009 variables\n\nacs09 = tidycensus::load_variables(2009, \"acs5\", cache = TRUE)\nacs09_Profile = load_variables(2009 , \"acs5/profile\", cache = TRUE)\nus_median_age09 <- get_acs(\n  geography = \"state\",\n  variables = \"B01002_001\",\n  year = 2009,\n  survey = \"acs1\",\n  geometry = TRUE,\n  resolution = \"20m\"\n) %>%\n  shift_geometry()\n\nplot(us_median_age09$geometry)\nggplot(data = us_median_age09, aes(fill = estimate)) + \n  geom_sf(col=\"green\") +  # Why color is white?\n  theme_bw() +\n  scale_fill_distiller(palette = \"PuBuGn\",  # Try other palette?\n                       direction = 1) + \n  labs(title = \"  Median Age by State, 2009\",\n       caption = \"Data source: 2009 1-year ACS, US Census Bureau\",\n       fill = \"\", family=\"Palatino\") +\n  theme(legend.position=c(.08,.6), legend.direction=\"vertical\") +\n  theme(text = element_text(family = \"Palatino\"), plot.title = element_text(hjust = 0.5))\n```\n\n![](images/a8.1.png)\n\n![](images/a8.2.png)\n\n3.  There are states where we can observe age go up such as Nevada, Arizona, and New Mexico. We see a very similar trend in Puerto Rico. This may be due to the retirees heading to these locations.\n","srcMarkdownNoYaml":"\n\n# Assignment #1\n\n**ANALYZE SURVEY**\n\n**a-c**: The questions on the survey are relatively simple and non complex. Given that they relate to movie rentals they are particular to this topic. They begin with multiple choice/ranking questions and then are set up with multiple choice questions, but for the most part questions that are strictly qualitative.\n\nPersonally, my only criticism involves design. The questions seem long and from a survey design perspective not intuitive for mobile users.\n\n**d,3-7**: The survey is fairly intuitive and does not need editing, it does not deviate much from pre-existing surveys. Adding the UTD logo personalizes it to the university which makes it useful for UTD users.\n\n**FURTHER ASSIGNMENT**\n\n**1-4**: Inserting this block questions allows for the introduction of multiple questions at a time, assuming these blocks are preexisting they create an easier survey experience.\n\n# Assignment #2\n\nThe difference between the two programs comes down to intuition. If the purpose is to visualize data from a non social science research perspective GTrends is much better at doing this. However, if the goal is to both analyze and perform research the gtrends package is much better.\n\nIt would be interesting to perform an analysis importing the csv from google versus using the gtrends package.\n\n# Assignment #3\n\n**3a.**: The Biden-Xi analysis allows for an understanding of what is going on within United States international relations and what revolves around the conversation. Terms like **coronavirus** and **fentanyl** stand out, because of the controversy surrounding COVID-19, as well as the flow of fentanyl into the United States that is in part due to the drug coming from the souther border being of Chinese origin. In addition to the aforementioned terms it is important to note the role of human rights and the tension between rising Chinese hegemony and the United States hegemonic position being challenged by it. Since the U.S. represents Western Liberal democratic order it is possible to see how **human rights** ties into the crises involving both Tibetans and Uyghers in China.\n\n![](images/3.1.png)\n\n**3b.**: Looking over time, it is important to note that at least within the plot there is very little change in the use of the term \"american\" and this is very similar to the the use of the word \"people\". Across time, these terms do not look like they change across time. There are however a couple of anomalies within the plot - The Trump 2017 speach being one of these instances, where his use of the term \"american\" is greater than everybody else. An explanation for this may be due to his populist platform that appeases to his voter base. A similar thing is found with people in both Johnson and Nixon regarding people, but a qualitative analysis may be necessary to know why they used these terms to the extent they did. For analyses regarding ideology it would be interesting to do a similar analysis between the term \"communist\" and \"socialist\" - I attempted an analysis and was not able to get a plot for it.\n\n![](images/3.2.png)\n\n# Assignment #5\n\n1.  There were issues in my ability to access this data. However, listed below is what it looked like. As you can tell there is an original ID and secret. Issues regarding third-party apps made it difficult to continue to the data analysis portion of the assignment.\n\n```         \n\\# = Autenthication = \\# \\# \\# yt_oauth(\"207302960577-7uqd83qilb3586f0dj77p2h4hq06kjs0.apps.googleusercontent.com\",\"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\", token = \"\")\n\nyt_israelprotest = yt_search(term = \"Israel protest\")\n\n# List of categories (region filter: US)\n\nvideocat_us= list_videocats(c(region_code = \"us\")) \\# = Download and prepare data = \\# mostpop = list_videos()\n\nmostpop_us = list_videos(video_category_id = \"25\", region_code = \"US\", max_results = 10)\n\n# Find the channel ID in the source page\n\n# Alternatively, from get_video_details\n\n# = Channel stats =\n\nnbcnews_stat = get_channel_stats(\"UCeY0bbntWzzVIaj2z3QigXg\") nbcnews_detail = get_video_details(video_id = \"to0YqKKRIWY\")\n\n# = Videos =\n\ncurl::curl_version() httr::set_config(httr::config(http_version = 0)) \\# Fix curl issue\n\nnbc_videos1 = yt_search(term=\"\", type=\"video\", channel_id = \"UCeY0bbntWzzVIaj2z3QigXg\") nbc_videos = nbc_videos1 %\\>% mutate(date = as.Date(publishedAt)) %\\>% filter(date \\> \"2022-11-27\") %\\>% arrange(date) samplecomment = get_comment_threads(c(video_id = \"to0YqKKRIWY\"), max_results = 600) samplecomment2 = get_all_comments(c(video_id = \"to0YqKKRIWY\"), max_results = 600) \\# = Comments, may take a long time \\# nbc_comments = lapply(as.character(nbc_videos1\\$video_id), function(x){ get_comment_threads(c(video_id = x), max_results = 101) }) \n```\n\na\\. An analysis of CNN would be the following. Keeping all things the same, except the channel ID.\n\n```         \n# = Autenthication = #\n# \n# \nyt_oauth(\"207302960577-7uqd83qilb3586f0dj77p2h4hq06kjs0.apps.googleusercontent.com\",\"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\", token = \"\")\n\nyt_israelprotest = yt_search(term = \"Israel protest\")\n\n# List of categories (region filter: US)\nvideocat_us= list_videocats(c(region_code = \"us\"))\n# = Download and prepare data = #\nmostpop = list_videos()\n\nmostpop_us = list_videos(video_category_id = \"25\",   region_code = \"US\", max_results = 10)\n\n\n# Find the channel ID in the source page\n# Alternatively, from get_video_details\n# = Channel stats = #\n\ncnn_stat = get_channel_stats(\"UCeY0bbntWzzVIaj2z3QigXg\")\ncnn_detail = get_video_details(video_id = \"to0YqKKRIWY\")\n\n\n\n# = Videos = #\ncurl::curl_version()\nhttr::set_config(httr::config(http_version = 0)) # Fix curl issue\n\ncnn_videos1 = yt_search(term=\"\", type=\"video\", channel_id = \"UCeY0bbntWzzVIaj2z3QigXg\")\ncnn_videos = nbc_videos1 %>%\n  mutate(date = as.Date(publishedAt)) %>%\n  filter(date > \"2022-11-27\") %>%\n  arrange(date)\nsamplecomment = get_comment_threads(c(video_id = \"to0YqKKRIWY\"), max_results = 600)\nsamplecomment2 = get_all_comments(c(video_id = \"to0YqKKRIWY\"), max_results = 600)\n# = Comments, may take a long time #\ncnn_comments = lapply(as.character(nbc_videos1$video_id), function(x){\n  get_comment_threads(c(video_id = x), max_results = 101)\n})\n```\n\n3.  I can potentially use quanteda, but given the difficulty accessing third party apps it does not seem possible. It would be interesting to see a data scientist analyzing their own child do this.\n\n# Assignment #6\n\n1.  These are the wordclouds from the rscript textmining and the code used\n\n```         \n \\# Data Method: Text mining \\# File: textmining1.R \\# Theme: Download text data from web and create wordcloud\n\n    # Install the easypackages package\n\n    install.packages(\"easypackages\") library(easypackages)\n\n    # Download text data from website\n\n    mlkLocation \\<-URLencode(\"http://www.analytictech.com/mb021/mlk.htm\")\n\n    # use htmlTreeParse function to read and parse paragraphs\n\n    doc.html\\<- htmlTreeParse(mlkLocation, useInternal=TRUE) mlk \\<- unlist(xpathApply(doc.html, '//p', xmlValue)) mlk head(mlk, 3)\n\n    # Vectorize mlk\n\n    words.vec \\<- VectorSource(mlk)\n\n    # Check the class of words.vec\n\n    class(words.vec)\n\n    # Create Corpus object for preprocessing\n\n    words.corpus \\<- Corpus(words.vec) inspect(words.corpus)\n\n    # Turn all words to lower case\n\n    words.corpus \\<- tm_map(words.corpus, content_transformer(tolower))\n\n    # Remove punctuations, numbers\n\n    words.corpus \\<- tm_map(words.corpus, removePunctuation) words.corpus \\<- tm_map(words.corpus, removeNumbers)\n\n    # How about stopwords, then uniform bag of words created\n\n    words.corpus \\<- tm_map(words.corpus, removeWords, stopwords(\"english\"))\n\n    # Create Term Document Matrix\n\n    tdm \\<- TermDocumentMatrix(words.corpus) inspect(tdm)\n\n    m \\<- as.matrix(tdm) wordCounts \\<- rowSums(m) wordCounts \\<- sort(wordCounts, decreasing=TRUE) head(wordCounts)\n\n    # Create Wordcloud\n\n    cloudFrame\\<-data.frame(word=names(wordCounts),freq=wordCounts)\n\n    set.seed(1234) wordcloud(cloudFrame$word,cloudFrame$freq) wordcloud(names(wordCounts),wordCounts, min.freq=3,random.order=FALSE, max.words=500,scale=c(3,.5), rot.per=0.35,colors=brewer.pal(8,\"Dark2\"))\n    \n```\n\n![](images/a6.2.png)\n\n![](images/a6.1.png)\n\nThe following is from the Winston Churchill speech, code and images\n\n```         \n \\# Download text data from website wcLocation \\<-URLencode(\"http://www.historyplace.com/speeches/churchill-hour.htm\")\n\n# use htmlTreeParse function to read and parse paragraphs\n\ndoc.html\\<- htmlTreeParse(wcLocation, useInternal=TRUE) wc \\<- unlist(xpathApply(doc.html, '//p', xmlValue)) wc head(wc, 3)\n\n# Vectorize wc\n\nwords.vec \\<- VectorSource(wc)\n\n# Check the class of words.vec\n\nclass(words.vec)\n\n# Create Corpus object for preprocessing\n\nwords.corpus \\<- Corpus(words.vec) inspect(words.corpus)\n\n# Turn all words to lower case\n\nwords.corpus \\<- tm_map(words.corpus, content_transformer(tolower))\n\n# Remove punctuations, numbers\n\nwords.corpus \\<- tm_map(words.corpus, removePunctuation) words.corpus \\<- tm_map(words.corpus, removeNumbers)\n\n# How about stopwords, then uniform bag of words created\n\nwords.corpus \\<- tm_map(words.corpus, removeWords, stopwords(\"english\"))\n\n# Create Term Document Matrix\n\ntdm \\<- TermDocumentMatrix(words.corpus) inspect(tdm)\n\nm \\<- as.matrix(tdm) wordCounts \\<- rowSums(m) wordCounts \\<- sort(wordCounts, decreasing=TRUE) head(wordCounts)\n\n# Create Wordcloud\n\ncloudFrame\\<-data.frame(word=names(wordCounts),freq=wordCounts)\n\nset.seed(1234) wordcloud(cloudFrame$word,cloudFrame$freq) wordcloud(names(wordCounts),wordCounts, min.freq=3,random.order=FALSE, max.words=500,scale=c(3,.5), rot.per=0.35,colors=brewer.pal(8,\"Dark2\")) \\~\\~\\~\n```\n\n![](images/a6.3.png)\n\n![](images/a6.4.png)\n\nCode for running rvest01\n\n```         \n## Workshop: Scraping webpages with R rvest package \\# Prerequisites: Chrome browser, Selector Gadget\n\n#install.packages(\"tidyverse\") library(tidyverse) #install.packages(\"rvest\") library(rvest)\n\nurl \\<- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves' #Reading the HTML code from the Wiki website wikiforreserve \\<- read_html(url) class(wikiforreserve)\n\n## Get the XPath data using Inspect element feature in Safari, Chrome or Firefox\n\n\\## At Inspect tab, look for\n\n<table class=....>\n\ntag. Leave the table close \\## Right click the table and Copy XPath, paste at html_nodes(xpath =)\n\nforeignreserve \\<- wikiforreserve %\\>% html_nodes(xpath='//\\*[@id=\"mw-content-text\"]/div/table\\[1\\]') %\\>% html_table() class(foreignreserve) fores = foreignreserve\\[\\[1\\]\\]\n\nnames(fores) \\<- c(\"Rank\", \"Country\", \"Forexres\", \"Date\", \"Change\", \"Sources\") colnames(fores)\n\nhead(fores\\$Country, n=10)\n\n## Clean up variables\n\n## What type is Rank?\n\n## How about Date?\n\n# Remove trailing notes in Date variable\n\nlibrary(stringr) fores$newdate = str_split_fixed(fores$Date, \"\\\\\\[\", n = 2)\\[, 1\\]\n\nwrite.csv(fores, \"fores.csv\", row.names = FALSE) \\~\\~\\~\n```\n\nImage of csv \"Fores\"\n\n![](images/a6.5.png)\n\nScript rvest02\n\n```         \n## Workshop: Scraping webpages with R rvest package \\# Prerequisites: Chrome browser, Selector Gadget\n\n# install.packages(\"tidyverse\")\n\nlibrary(tidyverse) \\# install.packages(\"rvest\") url1 = \"https://www.imdb.com/search/title/?release_date=2022-01-01,2023-01-01\" imdb2022 \\<- read_html(url1) rank_data_html \\<- html_nodes(imdb2022,'.text-primary') rank_data \\<- as.numeric(html_text(rank_data_html)) head(rank_data, n = 10) title_data_html \\<- html_nodes(imdb2022,'.lister-item-header a') title_data \\<- html_text(title_data_html)\n\nhead(title_data, n =20)\n```\n\nThe difficulty of this is that I am unsure as to how to make this visual.\n\n# Assignment #7\n\n1.  Code for first part of govdata with output\n\n```         \n    ## Scraping Government data\n    ## Website: GovInfo (https://www.govinfo.gov/app/search/)\n    ## Prerequisite: Download from website the list of files to be downloaded\n    ## Designed for background job\n\n    # Start with a clean plate and lean loading to save memory\n     \n    gc(reset=T)\n    rm(list = ls())\n\n    #install.packages(c(\"purrr\", \"magrittr\")\n    install.packages(\"purr\")\n    install.packages(\"magrittr\")\n    library(purrr)\n    library(magrittr)\n\n    ## Set path for reading the listing and home directory\n    ## For Windows, use \"c:\\\\directory\\\\subdirectory\"\n    library(readr)\n\n\n    govfiles= read_csv(\"govinfo-search-results-2023-12-05T16_37_50.csv\")\n    View(govfiles)\n\n\n    # Directory to save the pdf's\n    save_dir <- \"C:\\\\epps6302\\\\pdf\"\n\n    # Function to download pdfs\n    download_govfiles_pdf <- function(url, id) {\n      tryCatch({\n        destfile <- paste0(save_dir, \"govfiles_\", id, \".pdf\")\n        download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n        Sys.sleep(runif(1, 1, 3))  # Important: random sleep between 1 and 3 seconds to avoid suspicion of \"hacking\" the server\n        return(paste(\"Successfully downloaded:\", url))\n      },\n      error = function(e) {\n        return(paste(\"Failed to download:\", url))\n      })\n    }\n\n    # Download files, potentially in parallel for speed\n    # Simple timer, can use package like tictoc\n    start.time <- Sys.time()\n    message(\"Starting downloads\")\n    results <- 1:length(pdf_govfiles_url) %>% \n      purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\n    message(\"Finished downloads\")\n    end.time <- Sys.time()\n    time.taken <- end.time - start.time\n    time.taken\n\n    # Print results\n    print(results)\n```\n\n![](images/a7.1.png)\n\n118th Congress Congressional Hearings in Committee on Foreign Affairs csv\n\n![](images/a7.2.png)\n\nA main issue with the parallel R file is the issue of cores and difficulty with overcoming this and constantly receiving and error. Code Below and output as well\n\n```         \n## Parallelization\n## Prerequisite: Multiple core on CPU\n\n# Load the parallel package\nlibrary(parallel)\nlibrary(pdftools)\n\n# Create a function to be applied in parallelizing later jobs\nread_pdf_to_text <- function(uri) {\n  text <- pdftools::pdf_text(uri)\n  return(text)\n}\n# For mac\n\npdf_texts <- mclapply(pdfpath, read_pdf_to_text, mc.cores = num_cores)\ntoc()\n\n## For all platforms\n# Load the parallel package\nlibrary(parallel)\nlibrary(pdftools)\n# Get the number of cores available on your machine\nnum_cores <- detectCores()\n\n# Initialize a cluster with the number of available cores\ncl <- makeCluster(num_cores)\n\n# Load libraries and functions in each cluster\nclusterEvalQ(cl, library(pdftools))\n\n# Define the function to be parallelized after making the clusters\nread_pdf_to_text <- function(uri) {\n  text <- pdftools::pdf_text(uri)\n  return(text)\n}\n\n# Export any libraries or objects that will be used within the parallel code\n\n# Perform the parallel computation\ntic()\npdf_texts <- parLapply(cfa_ch, read_pdf_to_text, mc.cores = num_cores)\ntoc()\n\n# Don't forget to stop the cluster\nstopCluster(cl)\n```\n\n![](images/a7.3.png)\n\n# Assignment #8\n\n1.  A couple of observations regarding setting up the census data key. First, it is a relative simple process. However, the second, it is much faster if one does not use an .edu email.\n\nMy key is: 311dcd5892541c0eaa28747e5024f1e308c5xxxx\n\n2.  Below are 2019 and 2009 codes and images. Although the assignment asks for 2020, this is not available.\n\n```         \n\n# Get a list of American Community Survey (ACS) 2019 variables\nacs19 = tidycensus::load_variables(2019, \"acs5\", cache = TRUE)\nacs19_Profile = load_variables(2019 , \"acs5/profile\", cache = TRUE)\nus_median_age19 <- get_acs(\n  geography = \"state\",\n  variables = \"B01002_001\",\n  year = 2019,\n  survey = \"acs1\",\n  geometry = TRUE,\n  resolution = \"20m\"\n) %>%\n  shift_geometry()\n\nplot(us_median_age19$geometry)\nggplot(data = us_median_age19, aes(fill = estimate)) + \n  geom_sf(col=\"white\") +  # Why color is white?\n  theme_bw() +\n  scale_fill_distiller(palette = \"PuBuGn\",  # Try other palette?\n                       direction = 1) + \n  labs(title = \"  Median Age by State, 2019\",\n       caption = \"Data source: 2019 1-year ACS, US Census Bureau\",\n       fill = \"\", family=\"Palatino\") +\n  theme(legend.position=c(.08,.6), legend.direction=\"vertical\") +\n  theme(text = element_text(family = \"Palatino\"), plot.title = element_text(hjust = 0.5))\n\n# Get a list of American Community Survey (ACS) 2009 variables\n\nacs09 = tidycensus::load_variables(2009, \"acs5\", cache = TRUE)\nacs09_Profile = load_variables(2009 , \"acs5/profile\", cache = TRUE)\nus_median_age09 <- get_acs(\n  geography = \"state\",\n  variables = \"B01002_001\",\n  year = 2009,\n  survey = \"acs1\",\n  geometry = TRUE,\n  resolution = \"20m\"\n) %>%\n  shift_geometry()\n\nplot(us_median_age09$geometry)\nggplot(data = us_median_age09, aes(fill = estimate)) + \n  geom_sf(col=\"green\") +  # Why color is white?\n  theme_bw() +\n  scale_fill_distiller(palette = \"PuBuGn\",  # Try other palette?\n                       direction = 1) + \n  labs(title = \"  Median Age by State, 2009\",\n       caption = \"Data source: 2009 1-year ACS, US Census Bureau\",\n       fill = \"\", family=\"Palatino\") +\n  theme(legend.position=c(.08,.6), legend.direction=\"vertical\") +\n  theme(text = element_text(family = \"Palatino\"), plot.title = element_text(hjust = 0.5))\n```\n\n![](images/a8.1.png)\n\n![](images/a8.2.png)\n\n3.  There are states where we can observe age go up such as Nevada, Arizona, and New Mexico. We see a very similar trend in Puerto Rico. This may be due to the retirees heading to these locations.\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true,"format-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["styles.css"],"toc":true,"output-file":"assignments.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.3.450","editor":"visual","theme":"cosmo","title":"Assignments"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}