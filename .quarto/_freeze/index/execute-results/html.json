{
  "hash": "74d8107783341fb27ca7346d73ad3de6",
  "result": {
    "markdown": "---\ntitle: \"Tamnala Briggs-Megafu\"\n---\n\n\nThis is Tamnala Briggs-Megafu's website. Data projects are hosted here.\n\n\n::: {.cell}\n\n```{.r .cell-code}\npar (family=\"sans\")\nplot(iris, pch=20, cex=.75, col=\"steelblue\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\npar(family=\"Arial\")\n\n**Assignment #1**\n\n**1). QT Movie rental downloaded, and new project created.**\n\n<https://utdallas.yul1.qualtrics.com/survey-builder/SV_ewTQQWsSK3XjlMq/edit>\n\n**2). Analyze Survey**\n\na\\) **How is the survey structured** -- There is a page break after the first block, and the entire survey is made up of 18 blocks\n\nb\\) **What is the questionnaire composed of** -- The questionnaire is composed of text graphic, multiple choice, and matrix table type questions.\n\nc\\) **How are the questions ordered** -- The questionnaire starts with a recruitment letter followed by a page break. This is followed by 12 questions about customers' perceptions about the services received and followed by 4 questions about customers' demographics.\n\n3). **Change the look & feel to include UT Dallas Header -**\n\n4). **Use skip logic in Q4, if answer is NO, skip to the question Q7** \"Do you feel comfortable purchasing software over the internet? [What is the difference between Skip Logic and Display Logic?]{.underline}\n\n[**Skip logic**]{.underline} allows you to send the respondents to a future point in the survey based on how they answer a question while [**Display logic**]{.underline} will display the selected question if certain criteria are met.\n\n5\\. **Apply validation to every question - force response**\n\n6**. Insert page break to save respondents from scrolling down the screen**\n\n7\\) **Reorganize the survey by blocks** -- moved Q6 to be just after Q4\n\nii\\) **Apply display logic to one of the questions** -- Display logic applied to Q11 to reflect if customer selected ***already own a DVD player***from Q3\n\niii\\) **Try out the Carry Forward features**\n\niv\\) **How to make the survey mobile friendly?**\n\n1)     Add page breaks to every question -- page break added to every question to make the survey mobile friendly\n\n2)     Apply validation to every question -- Validation (force response) added to every question.\n\n3)     Reconsider Matrix table?\n\nv\\) Add-on challenges\n\n            1) Add default choices\n\n[**Default choices**]{.underline} can be used to feature answer choices within a question when respondents open a survey. This can be used when you want respondents to update previously collected data, such as updating their contact addresses or changes in their names if they get married for example. Also, respondents can use this feature to give suggestions on the survey.\n\n**Assignment #2**\n\nThe difference between the two programs comes down to intuition. If the purpose is to visualize data from a non social science research perspective GTrends is much better at doing this. However, if the goal is to both analyze and perform research the gtrends package is much better.\n\nIt would be interesting to perform an analysis importing the csv from google versus using the gtrends package.\n\n**Assignment #3**\n\n3a.: The Biden-Xi analysis allows for an understanding of what is going on within United States international relations and what revolves around the conversation. Terms like coronavirus and fentanyl stand out, because of the controversy surrounding COVID-19, as well as the flow of fentanyl into the United States that is in part due to the drug coming from the souther border being of Chinese origin. In addition to the aforementioned terms it is important to note the role of human rights and the tension between rising Chinese hegemony and the United States hegemonic position being challenged by it. Since the U.S. represents Western Liberal democratic order it is possible to see how human rights ties into the crises involving both Tibetans and Uyghers in China.\n\n3b.: Looking over time, it is important to note that at least within the plot there is very little change in the use of the term \"american\" and this is very similar to the the use of the word \"people\". Across time, these terms do not look like they change across time. There are however a couple of anomalies within the plot - The Trump 2017 speach being one of these instances, where his use of the term \"american\" is greater than everybody else. An explanation for this may be due to his populist platform that appeases to his voter base. A similar thing is found with people in both Johnson and Nixon regarding people, but a qualitative analysis may be necessary to know why they used these terms to the extent they did. For analyses regarding ideology it would be interesting to do a similar analysis between the term \"communist\" and \"socialist\" - I attempted an analysis and was not able to get a plot for it.\n\n**Assignment #5**\n\nThere were issues in my ability to access this data. However, listed below is what it looked like. As you can tell there is an original ID and secret. Issues regarding third-party apps made it difficult to continue to the data analysis portion of the assignment.\n\n\\# = Autenthication = \\# \\# \\# yt_oauth(\"207302960577-7uqd83qilb3586f0dj77p2h4hq06kjs0.apps.googleusercontent.com\",\"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\", token = \"\")\n\nyt_israelprotest = yt_search(term = \"Israel protest\")\n\n# List of categories (region filter: US)\n\nvideocat_us= list_videocats(c(region_code = \"us\")) \\# = Download and prepare data = \\# mostpop = list_videos()\n\nmostpop_us = list_videos(video_category_id = \"25\", region_code = \"US\", max_results = 10)\n\n# Find the channel ID in the source page\n\n# Alternatively, from get_video_details\n\n# = Channel stats =\n\nnbcnews_stat = get_channel_stats(\"UCeY0bbntWzzVIaj2z3QigXg\") nbcnews_detail = get_video_details(video_id = \"to0YqKKRIWY\")\n\n# = Videos =\n\ncurl::curl_version() httr::set_config(httr::config(http_version = 0)) \\# Fix curl issue\n\nnbc_videos1 = yt_search(term=\"\", type=\"video\", channel_id = \"UCeY0bbntWzzVIaj2z3QigXg\") nbc_videos = nbc_videos1 %\\>% mutate(date = as.Date(publishedAt)) %\\>% filter(date \\> \"2022-11-27\") %\\>% arrange(date) samplecomment = get_comment_threads(c(video_id = \"to0YqKKRIWY\"), max_results = 600) samplecomment2 = get_all_comments(c(video_id = \"to0YqKKRIWY\"), max_results = 600) \\# = Comments, may take a long time \\# nbc_comments = lapply(as.character(nbc_videos1\\$video_id), function(x){ get_comment_threads(c(video_id = x), max_results = 101) })\n\na.  An analysis of CNN would be the following. Keeping all things the same, except the channel ID.\n\n# = Autenthication =\n\n# \n\n# \n\nyt_oauth(\"207302960577-7uqd83qilb3586f0dj77p2h4hq06kjs0.apps.googleusercontent.com\",\"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\", token = \"\")\n\nyt_israelprotest = yt_search(term = \"Israel protest\")\n\n# List of categories (region filter: US)\n\nvideocat_us= list_videocats(c(region_code = \"us\")) \\# = Download and prepare data = \\# mostpop = list_videos()\n\nmostpop_us = list_videos(video_category_id = \"25\", region_code = \"US\", max_results = 10)\n\n# Find the channel ID in the source page\n\n# Alternatively, from get_video_details\n\n# = Channel stats =\n\ncnn_stat = get_channel_stats(\"UCeY0bbntWzzVIaj2z3QigXg\") cnn_detail = get_video_details(video_id = \"to0YqKKRIWY\")\n\n# = Videos =\n\ncurl::curl_version() httr::set_config(httr::config(http_version = 0)) \\# Fix curl issue\n\ncnn_videos1 = yt_search(term=\"\", type=\"video\", channel_id = \"UCeY0bbntWzzVIaj2z3QigXg\") cnn_videos = nbc_videos1 %\\>% mutate(date = as.Date(publishedAt)) %\\>% filter(date \\> \"2022-11-27\") %\\>% arrange(date) samplecomment = get_comment_threads(c(video_id = \"to0YqKKRIWY\"), max_results = 600) samplecomment2 = get_all_comments(c(video_id = \"to0YqKKRIWY\"), max_results = 600) \\# = Comments, may take a long time \\# cnn_comments = lapply(as.character(nbc_videos1\\$video_id), function(x){ get_comment_threads(c(video_id = x), max_results = 101) })\n\nI can potentially use quanteda, but given the difficulty accessing third party apps it does not seem possible. It would be interesting to see a data scientist analyzing their own child do this.\n\n**Assignment #6**\n\nThese are the wordclouds from the rscript textmining and the code used\n\n\\# Data Method: Text mining \\# File: textmining1.R \\# Theme: Download text data from web and create wordcloud\n\n```         \n# Install the easypackages package\n\ninstall.packages(\"easypackages\") library(easypackages)\n\n# Download text data from website\n\nmlkLocation \\<-URLencode(\"http://www.analytictech.com/mb021/mlk.htm\")\n\n# use htmlTreeParse function to read and parse paragraphs\n\ndoc.html\\<- htmlTreeParse(mlkLocation, useInternal=TRUE) mlk \\<- unlist(xpathApply(doc.html, '//p', xmlValue)) mlk head(mlk, 3)\n\n# Vectorize mlk\n\nwords.vec \\<- VectorSource(mlk)\n\n# Check the class of words.vec\n\nclass(words.vec)\n\n# Create Corpus object for preprocessing\n\nwords.corpus \\<- Corpus(words.vec) inspect(words.corpus)\n\n# Turn all words to lower case\n\nwords.corpus \\<- tm_map(words.corpus, content_transformer(tolower))\n\n# Remove punctuations, numbers\n\nwords.corpus \\<- tm_map(words.corpus, removePunctuation) words.corpus \\<- tm_map(words.corpus, removeNumbers)\n\n# How about stopwords, then uniform bag of words created\n\nwords.corpus \\<- tm_map(words.corpus, removeWords, stopwords(\"english\"))\n\n# Create Term Document Matrix\n\ntdm \\<- TermDocumentMatrix(words.corpus) inspect(tdm)\n\nm \\<- as.matrix(tdm) wordCounts \\<- rowSums(m) wordCounts \\<- sort(wordCounts, decreasing=TRUE) head(wordCounts)\n\n# Create Wordcloud\n\ncloudFrame\\<-data.frame(word=names(wordCounts),freq=wordCounts)\n\nset.seed(1234) wordcloud(cloudFrame$word,cloudFrame$freq) wordcloud(names(wordCounts),wordCounts, min.freq=3,random.order=FALSE, max.words=500,scale=c(3,.5), rot.per=0.35,colors=brewer.pal(8,\"Dark2\"))\n```\n\nThe following is from the Winston Churchill speech, code and images\n\n\\# Download text data from website wcLocation \\<-URLencode(\"http://www.historyplace.com/speeches/churchill-hour.htm\")\n\n# use htmlTreeParse function to read and parse paragraphs\n\ndoc.html\\<- htmlTreeParse(wcLocation, useInternal=TRUE) wc \\<- unlist(xpathApply(doc.html, '//p', xmlValue)) wc head(wc, 3)\n\n# Vectorize wc\n\nwords.vec \\<- VectorSource(wc)\n\n# Check the class of words.vec\n\nclass(words.vec)\n\n# Create Corpus object for preprocessing\n\nwords.corpus \\<- Corpus(words.vec) inspect(words.corpus)\n\n# Turn all words to lower case\n\nwords.corpus \\<- tm_map(words.corpus, content_transformer(tolower))\n\n# Remove punctuations, numbers\n\nwords.corpus \\<- tm_map(words.corpus, removePunctuation) words.corpus \\<- tm_map(words.corpus, removeNumbers)\n\n# How about stopwords, then uniform bag of words created\n\nwords.corpus \\<- tm_map(words.corpus, removeWords, stopwords(\"english\"))\n\n# Create Term Document Matrix\n\ntdm \\<- TermDocumentMatrix(words.corpus) inspect(tdm)\n\nm \\<- as.matrix(tdm) wordCounts \\<- rowSums(m) wordCounts \\<- sort(wordCounts, decreasing=TRUE) head(wordCounts)\n\n# Create Wordcloud\n\ncloudFrame\\<-data.frame(word=names(wordCounts),freq=wordCounts)\n\nset.seed(1234) wordcloud(cloudFrame$word,cloudFrame$freq) wordcloud(names(wordCounts),wordCounts, min.freq=3,random.order=FALSE, max.words=500,scale=c(3,.5), rot.per=0.35,colors=brewer.pal(8,\"Dark2\")) \\~\\~\\~\n\nCode for running rvest01\n\n## Workshop: Scraping webpages with R rvest package \\# Prerequisites: Chrome browser, Selector Gadget\n\n#install.packages(\"tidyverse\") library(tidyverse) #install.packages(\"rvest\") library(rvest)\n\nurl \\<- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves' #Reading the HTML code from the Wiki website wikiforreserve \\<- read_html(url) class(wikiforreserve)\n\n## Get the XPath data using Inspect element feature in Safari, Chrome or Firefox\n\n\\## At Inspect tab, look for\n\n<table class=....>\n\ntag. Leave the table close \\## Right click the table and Copy XPath, paste at html_nodes(xpath =)\n\nforeignreserve \\<- wikiforreserve %\\>% html_nodes(xpath='//\\*[@id=\"mw-content-text\"]/div/table\\[1\\]') %\\>% html_table() class(foreignreserve) fores = foreignreserve\\[\\[1\\]\\]\n\nnames(fores) \\<- c(\"Rank\", \"Country\", \"Forexres\", \"Date\", \"Change\", \"Sources\") colnames(fores)\n\nhead(fores\\$Country, n=10)\n\n## Clean up variables\n\n## What type is Rank?\n\n## How about Date?\n\n# Remove trailing notes in Date variable\n\nlibrary(stringr) fores$newdate = str_split_fixed(fores$Date, \"\\\\\\[\", n = 2)\\[, 1\\]\n\nwrite.csv(fores, \"fores.csv\", row.names = FALSE) \\~\\~\\~\n\nImage of csv \"Fores\"\n\nScript rvest02\n\n## Workshop: Scraping webpages with R rvest package \\# Prerequisites: Chrome browser, Selector Gadget\n\n# install.packages(\"tidyverse\")\n\nlibrary(tidyverse) \\# install.packages(\"rvest\") url1 = \"https://www.imdb.com/search/title/?release_date=2022-01-01,2023-01-01\" imdb2022 \\<- read_html(url1) rank_data_html \\<- html_nodes(imdb2022,'.text-primary') rank_data \\<- as.numeric(html_text(rank_data_html)) head(rank_data, n = 10) title_data_html \\<- html_nodes(imdb2022,'.lister-item-header a') title_data \\<- html_text(title_data_html)\n\nhead(title_data, n =20)\n\nThe difficulty of this is that I am unsure as to how to make this visual.\n\n**Assignment #7**\n\nCode for first part of govdata with output\n\n```         \n## Scraping Government data\n## Website: GovInfo (https://www.govinfo.gov/app/search/)\n## Prerequisite: Download from website the list of files to be downloaded\n## Designed for background job\n\n# Start with a clean plate and lean loading to save memory\n \ngc(reset=T)\nrm(list = ls())\n\n#install.packages(c(\"purrr\", \"magrittr\")\ninstall.packages(\"purr\")\ninstall.packages(\"magrittr\")\nlibrary(purrr)\nlibrary(magrittr)\n\n## Set path for reading the listing and home directory\n## For Windows, use \"c:\\\\directory\\\\subdirectory\"\nlibrary(readr)\n\n\ngovfiles= read_csv(\"govinfo-search-results-2023-12-05T16_37_50.csv\")\nView(govfiles)\n\n\n# Directory to save the pdf's\nsave_dir <- \"C:\\\\epps6302\\\\pdf\"\n\n# Function to download pdfs\ndownload_govfiles_pdf <- function(url, id) {\n  tryCatch({\n    destfile <- paste0(save_dir, \"govfiles_\", id, \".pdf\")\n    download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n    Sys.sleep(runif(1, 1, 3))  # Important: random sleep between 1 and 3 seconds to avoid suspicion of \"hacking\" the server\n    return(paste(\"Successfully downloaded:\", url))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", url))\n  })\n}\n\n# Download files, potentially in parallel for speed\n# Simple timer, can use package like tictoc\nstart.time <- Sys.time()\nmessage(\"Starting downloads\")\nresults <- 1:length(pdf_govfiles_url) %>% \n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\nmessage(\"Finished downloads\")\nend.time <- Sys.time()\ntime.taken <- end.time - start.time\ntime.taken\n\n# Print results\nprint(results)\n```\n\n118th Congress Congressional Hearings in Committee on Foreign Affairs csv\n\nA main issue with the parallel R file is the issue of cores and difficulty with overcoming this and constantly receiving and error. Code Below and output as well\n\n## Parallelization\n\n## Prerequisite: Multiple core on CPU\n\n# Load the parallel package\n\nlibrary(parallel) library(pdftools)\n\n# Create a function to be applied in parallelizing later jobs\n\nread_pdf_to_text \\<- function(uri) { text \\<- pdftools::pdf_text(uri) return(text) } \\# For mac\n\npdf_texts \\<- mclapply(pdfpath, read_pdf_to_text, mc.cores = num_cores) toc()\n\n## For all platforms\n\n# Load the parallel package\n\nlibrary(parallel) library(pdftools) \\# Get the number of cores available on your machine num_cores \\<- detectCores()\n\n# Initialize a cluster with the number of available cores\n\ncl \\<- makeCluster(num_cores)\n\n# Load libraries and functions in each cluster\n\nclusterEvalQ(cl, library(pdftools))\n\n# Define the function to be parallelized after making the clusters\n\nread_pdf_to_text \\<- function(uri) { text \\<- pdftools::pdf_text(uri) return(text) }\n\n# Export any libraries or objects that will be used within the parallel code\n\n# Perform the parallel computation\n\ntic() pdf_texts \\<- parLapply(cfa_ch, read_pdf_to_text, mc.cores = num_cores) toc()\n\n# Don't forget to stop the cluster\n\nstopCluster(cl)\n\n**Assignment #8**\n\nA couple of observations regarding setting up the census data key. First, it is a relative simple process. However, the second, it is much faster if one does not use an .edu email.\n\nMy key is: 311dcd5892541c0eaa28747e5024f1e308c5xxxx\n\nBelow are 2019 and 2009 codes and images. Although the assignment asks for 2020, this is not available.\n\n# Get a list of American Community Survey (ACS) 2019 variables\n\nacs19 = tidycensus::load_variables(2019, \"acs5\", cache = TRUE) acs19_Profile = load_variables(2019 , \"acs5/profile\", cache = TRUE) us_median_age19 \\<- get_acs( geography = \"state\", variables = \"B01002_001\", year = 2019, survey = \"acs1\", geometry = TRUE, resolution = \"20m\" ) %\\>% shift_geometry()\n\nplot(us_median_age19\\$geometry) ggplot(data = us_median_age19, aes(fill = estimate)) + geom_sf(col=\"white\") + \\# Why color is white? theme_bw() + scale_fill_distiller(palette = \"PuBuGn\", \\# Try other palette? direction = 1) + labs(title = \" Median Age by State, 2019\", caption = \"Data source: 2019 1-year ACS, US Census Bureau\", fill = \"\", family=\"Palatino\") + theme(legend.position=c(.08,.6), legend.direction=\"vertical\") + theme(text = element_text(family = \"Palatino\"), plot.title = element_text(hjust = 0.5))\n\n# Get a list of American Community Survey (ACS) 2009 variables\n\nacs09 = tidycensus::load_variables(2009, \"acs5\", cache = TRUE) acs09_Profile = load_variables(2009 , \"acs5/profile\", cache = TRUE) us_median_age09 \\<- get_acs( geography = \"state\", variables = \"B01002_001\", year = 2009, survey = \"acs1\", geometry = TRUE, resolution = \"20m\" ) %\\>% shift_geometry()\n\nplot(us_median_age09\\$geometry) ggplot(data = us_median_age09, aes(fill = estimate)) + geom_sf(col=\"green\") + \\# Why color is white? theme_bw() + scale_fill_distiller(palette = \"PuBuGn\", \\# Try other palette? direction = 1) + labs(title = \" Median Age by State, 2009\", caption = \"Data source: 2009 1-year ACS, US Census Bureau\", fill = \"\", family=\"Palatino\") + theme(legend.position=c(.08,.6), legend.direction=\"vertical\") + theme(text = element_text(family = \"Palatino\"), plot.title = element_text(hjust = 0.5))\n\nThere are states where we can observe age go up such as Nevada, Arizona, and New Mexico. We see a very similar trend in Puerto Rico. This may be due to the retirees heading to these locations.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}