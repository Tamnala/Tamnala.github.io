[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "1 + 1\n\n[1] 2"
  },
  {
    "objectID": "assignment1.html",
    "href": "assignment1.html",
    "title": "assignment1",
    "section": "",
    "text": "Q1.\nThe example link here.\n\n# Title Fall color\n# Credit: https://fronkonstin.com\n\n# Install packages\n\n#install.packages(\"gsubfn\")\n#install.packages(\"tidyverse\")\nlibrary(gsubfn)\n\nWarning: package 'gsubfn' was built under R version 4.3.3\n\n\nLoading required package: proto\n\n\nWarning: package 'proto' was built under R version 4.3.3\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.3.3\n\n\nWarning: package 'ggplot2' was built under R version 4.3.3\n\n\nWarning: package 'tibble' was built under R version 4.3.3\n\n\nWarning: package 'tidyr' was built under R version 4.3.3\n\n\nWarning: package 'readr' was built under R version 4.3.3\n\n\nWarning: package 'purrr' was built under R version 4.3.3\n\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\nWarning: package 'stringr' was built under R version 4.3.3\n\n\nWarning: package 'forcats' was built under R version 4.3.3\n\n\nWarning: package 'lubridate' was built under R version 4.3.3\n\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.0     ✔ tibble    3.2.1\n✔ lubridate 1.9.3     ✔ tidyr     1.3.1\n✔ purrr     1.0.2     \n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n# Define elements in plant art\n# Each image corresponds to a different axiom, rules, angle and depth\n\n# Leaf of Fall\n\naxiom=\"X\"\nrules=list(\"X\"=\"F-[[X]+X]+F[+FX]-X\", \"F\"=\"FF\")\nangle=22.5\ndepth=6\n\n\nfor (i in 1:depth) axiom=gsubfn(\".\", rules, axiom)\n\nactions=str_extract_all(axiom, \"\\\\d*\\\\+|\\\\d*\\\\-|F|L|R|\\\\[|\\\\]|\\\\|\") %&gt;% unlist\n\nstatus=data.frame(x=numeric(0), y=numeric(0), alfa=numeric(0))\npoints=data.frame(x1 = 0, y1 = 0, x2 = NA, y2 = NA, alfa=90, depth=1)\n\n\n# Generating data\n# Note: may take a minute or two\n\nfor (action in actions)\n{\n  if (action==\"F\")\n  {\n    x=points[1, \"x1\"]+cos(points[1, \"alfa\"]*(pi/180))\n    y=points[1, \"y1\"]+sin(points[1, \"alfa\"]*(pi/180))\n    points[1,\"x2\"]=x\n    points[1,\"y2\"]=y\n    data.frame(x1 = x, y1 = y, x2 = NA, y2 = NA,\n               alfa=points[1, \"alfa\"],\n               depth=points[1,\"depth\"]) %&gt;% rbind(points)-&gt;points\n  }\n  if (action %in% c(\"+\", \"-\")){\n    alfa=points[1, \"alfa\"]\n    points[1, \"alfa\"]=eval(parse(text=paste0(\"alfa\",action, angle)))\n  }\n  if(action==\"[\"){\n    data.frame(x=points[1, \"x1\"], y=points[1, \"y1\"], alfa=points[1, \"alfa\"]) %&gt;%\n      rbind(status) -&gt; status\n    points[1, \"depth\"]=points[1, \"depth\"]+1\n  }\n  \n  if(action==\"]\"){\n    depth=points[1, \"depth\"]\n    points[-1,]-&gt;points\n    data.frame(x1=status[1, \"x\"], y1=status[1, \"y\"], x2=NA, y2=NA,\n               alfa=status[1, \"alfa\"],\n               depth=depth-1) %&gt;%\n      rbind(points) -&gt; points\n    status[-1,]-&gt;status\n  }\n}\n\nggplot() +\n  geom_segment(aes(x = x1, y = y1, xend = x2, yend = y2),\n               lineend = \"round\",\n               color=\"red\", # Set your own Fall color?\n               data=na.omit(points)) +\n  coord_fixed(ratio = 1) +\n  theme_void() # No grid nor axes\n\n\n\n\nQ3\n\nThis image provides a guide to the reason why inflation remains high in America."
  },
  {
    "objectID": "assignment2.html",
    "href": "assignment2.html",
    "title": "assignment2",
    "section": "",
    "text": "Q2.\n\n#########################################\nrm(list=ls())                          # Clear environment\n\n\nlibrary(readr)\n\nWarning: package 'readr' was built under R version 4.3.3\n\nhpi_df &lt;- read_csv(\"HPI.csv\")\n\nRows: 1064 Columns: 12\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (2): Country, ISO\ndbl (10): HPI rank, year, Continent, Population (thousands), Life Expectancy...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ncolnames(hpi_df)&lt;-c(\"hpi_rank\",\"country\",\"iso\",\"year\",\n                    \"continent\", \"population\", \"life_expectancy\",\n                    \"ladder_of_life\", \"ecological_footprint\",\n                    \"hpi\", \"biocapacity_for_year\", \"gdp\")\n\nhpi2_df &lt;- na.omit(hpi_df)\n\n## Start plotting from basics \n#  plotting functions.\nyear &lt;- c(2013, 2014, 2015, 2016, 2017, 2018, 2019)\nhpi_score &lt;- aggregate(hpi ~ year, hpi2_df, mean)\nlife_score  &lt;- aggregate(life_expectancy ~ year, hpi2_df, mean)\nhpi_score &lt;- c(hpi_score$hpi)\nlife_score &lt;- c(life_score$life_expectancy)\n\n# Setting the parameter (3 rows by 2 cols)\npar(mfrow=c(3, 2))\n\n# Setting label orientation, margins c(bottom, left, top, right) & text size\npar(las=1, mar=c(4, 4, 2, 4), cex=.7) \nplot.new()\nplot.window(range(year), c(40, 80))\nlines(year, hpi_score)\nlines(year, life_score)\npoints(year, hpi_score, pch=17, bg = \"blue\", cex=1) # Try different cex value?  \npoints(year, life_score, pch=21, bg =\"red\", cex=1)  # Different background color\npar(col=\"gray50\", fg=\"gray50\", col.axis=\"gray50\")\naxis(1, at=seq(2013, 2019, 2)) # What is the first number standing for?\naxis(2, at=seq(40, 80, 10))\naxis(4, at=seq(40, 80, 10))\nbox(bty=\"u\")\nmtext(\"year\", side=1, line=2, cex=0.8)\nmtext(\"HPI\", side=2, line=2, las=0, cex=0.8)\nmtext(\"Life Exp\", side=4, line=2, las=0, cex=0.8)\ntext(4, 5, \"Bird 131\")\npar(mar=c(5.1, 4.1, 4.1, 2.1), col=\"black\", fg=\"black\", col.axis=\"black\")\n\n######## Histogram ########\n\n# Make sure no Y exceed [-3.5, 3.5]\npar(mar=c(4.5, 4.1, 3.1, 0))\nhist(hpi2_df$hpi, breaks=seq(floor(min(hpi2_df$hpi)),\n                             ceiling(max(hpi2_df$hpi))), \n     main=\"Height histogram\", xlab=\"HPI\", \n     col=\"gray80\", freq=FALSE)\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n####### Barplot ######\nlibrary(dplyr)\n\nWarning: package 'dplyr' was built under R version 4.3.3\n\n\n\nAttaching package: 'dplyr'\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\nsummary(hpi2_df$gdp)\n\n    Min.  1st Qu.   Median     Mean  3rd Qu.     Max. \n   751.7   4984.0  12874.5  21034.2  31201.9 114304.0 \n\nhpi3_df &lt;- hpi2_df %&gt;%\n  mutate(gdp_lev = ifelse(hpi2_df$gdp &gt;= 31202, 'high',\n                          ifelse(hpi2_df$gdp &gt;= 4985, 'middle',\n                                 'low')))\n\nsummary(hpi2_df$hpi)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  22.35   38.32   44.24   44.05   49.88   64.73 \n\nhpi3_df &lt;- hpi3_df %&gt;%\n  mutate(hpi_lev = ifelse(hpi2_df$hpi &gt; 49, 'good',\n                          ifelse(hpi2_df$hpi &gt; 39, 'average',\n                                 'bad')))\n\ngdp_sub_h &lt;- subset(hpi3_df, gdp_lev == \"high\")\naggregate(gdp_lev ~ hpi_lev, gdp_sub_h, length)\n\n  hpi_lev gdp_lev\n1 average     122\n2     bad      50\n3    good      75\n\ngdp_sub_m &lt;- subset(hpi3_df, gdp_lev == \"middle\")\naggregate(gdp_lev ~ hpi_lev, gdp_sub_m, length)\n\n  hpi_lev gdp_lev\n1 average     223\n2     bad      96\n3    good     175\n\ngdp_sub_l &lt;- subset(hpi3_df, gdp_lev == \"low\")\naggregate(gdp_lev ~ hpi_lev, gdp_sub_l, length)\n\n  hpi_lev gdp_lev\n1 average      85\n2     bad     134\n3    good      28\n\nhpi_le &lt;- c(\"average\", \"bad\", \"good\")\nhigh &lt;- c(122, 50, 75)\nmiddle &lt;- c(223, 96, 175)\nlow &lt;- c(85, 134, 28)\n\nbar_df &lt;- data.frame(high, middle, low)\nrownames(bar_df)=hpi_le\n\npar(mar=c(2, 3.1, 2, 2.1))\nmidpts &lt;- barplot(as.matrix(bar_df), names = rep(\"\", 3),\n                  col = c(\"yellow\", \"pink\", \"blue\"))\nlegend(\"topright\", inset=.02,\n       c(\"average\",\"bad\", \"good\"), fill = c(\"yellow\", \"pink\", \"blue\"), \n       horiz=FALSE, cex=0.5)\nmtext(\"GDP per capita\", side=3, line=0.0, cex=0.5)\nmtext(sub(\" \", \"\\n\", colnames(bar_df)),\n      at=midpts, side=1, line=0.5, cex=0.5)\n\npar(mar=c(5.1, 4.1, 4.1, 2.1))  \n\n\n###### Boxplot ######\npar(mar=c(2, 4, 1, 0.5))\nboxplot(hpi3_df$hpi ~ hpi3_df$gdp_lev, data = hpi3_df,\n        boxwex = 0.4, at = 1:3 - 0.2,\n        subset= hpi3_df$hpi_lev == \"good\", col=\"blue\",\n        xlab=\"\",\n        ylab=\"HPI\", ylim=c(20,70))\nmtext(\"GDP per capita\", side=1, line=1.8, cex=0.5)\n\nboxplot(hpi3_df$hpi ~ hpi3_df$gdp_lev, data = hpi3_df, add = TRUE,\n        boxwex = 0.4, at = 1:3 + 0.2,\n        subset= hpi3_df$hpi_lev == \"bad\", col=\"red\")\nlegend(\"bottomleft\", inset=.02,\n       c(\"good\",\"bad\"), fill = c(\"blue\", \"red\"), horiz=TRUE, cex=0.5)\npar(mar=c(5.1, 4.1, 4.1, 2.1))\n\n###### Persp ######\npar(mar=c(0.5, 0.5, 0, 0), lwd=0.5)\nx &lt;- y &lt;- seq(-10, 10, length = 50);\nz &lt;- outer(x, y,\n           function(x,y) {\n             r &lt;- sqrt(x^2 + y^2)+3;\n             cos(r)/r\n           });\n\npersp(x, y, z,\n      theta  = 20,        # Rotation about z-axis, in degrees\n      phi    = 30,        # Rotation about x-axis, in degrees\n      expand = 0.5,        # Shrinking/growing of z values\n      shade  = 0.3)\n\npar(mar=c(5.1, 4.1, 4.1, 2.1), lwd=1)\n\n\n# Piechart\npar(mar=c(0, 2, 1, 2), xpd=FALSE, cex=0.5)\npie.sales &lt;- c(0.11, 0.06, 0.23, 0.11, 0.12, 0.37)\nnames(pie.sales) &lt;- c(\"Africa\", \"Asia\",\n                      \"Europe\", \"Oceania\", \"Others\", \"S.America\")\npie(pie.sales, col = rainbow(7))"
  },
  {
    "objectID": "assignment3.html",
    "href": "assignment3.html",
    "title": "assignment3",
    "section": "",
    "text": "Q1.\n\n# Data Visualization HW 3\n\n# Tamnala \n\n# Fall 2022\n\n\nrm(list=ls())\n\ndata(anscombe)  # Load Anscombe's data\n\nView(anscombe) # View the data\n\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n## Simple version\nplot(anscombe$x1,anscombe$y1)\n\nsummary(anscombe)\n\n       x1             x2             x3             x4           y1        \n Min.   : 4.0   Min.   : 4.0   Min.   : 4.0   Min.   : 8   Min.   : 4.260  \n 1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 6.5   1st Qu.: 8   1st Qu.: 6.315  \n Median : 9.0   Median : 9.0   Median : 9.0   Median : 8   Median : 7.580  \n Mean   : 9.0   Mean   : 9.0   Mean   : 9.0   Mean   : 9   Mean   : 7.501  \n 3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.:11.5   3rd Qu.: 8   3rd Qu.: 8.570  \n Max.   :14.0   Max.   :14.0   Max.   :14.0   Max.   :19   Max.   :10.840  \n       y2              y3              y4        \n Min.   :3.100   Min.   : 5.39   Min.   : 5.250  \n 1st Qu.:6.695   1st Qu.: 6.25   1st Qu.: 6.170  \n Median :8.140   Median : 7.11   Median : 7.040  \n Mean   :7.501   Mean   : 7.50   Mean   : 7.501  \n 3rd Qu.:8.950   3rd Qu.: 7.98   3rd Qu.: 8.190  \n Max.   :9.260   Max.   :12.74   Max.   :12.500  \n\n# Create four model objects\nlm1 &lt;- lm(y1 ~ x1, data=anscombe)\nsummary(lm1)\n\n\nCall:\nlm(formula = y1 ~ x1, data = anscombe)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.92127 -0.45577 -0.04136  0.70941  1.83882 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0001     1.1247   2.667  0.02573 * \nx1            0.5001     0.1179   4.241  0.00217 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6665,    Adjusted R-squared:  0.6295 \nF-statistic: 17.99 on 1 and 9 DF,  p-value: 0.00217\n\nlm2 &lt;- lm(y2 ~ x2, data=anscombe)\nsummary(lm2)\n\n\nCall:\nlm(formula = y2 ~ x2, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.9009 -0.7609  0.1291  0.9491  1.2691 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)    3.001      1.125   2.667  0.02576 * \nx2             0.500      0.118   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.237 on 9 degrees of freedom\nMultiple R-squared:  0.6662,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002179\n\nlm3 &lt;- lm(y3 ~ x3, data=anscombe)\nsummary(lm3)\n\n\nCall:\nlm(formula = y3 ~ x3, data = anscombe)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.1586 -0.6146 -0.2303  0.1540  3.2411 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0025     1.1245   2.670  0.02562 * \nx3            0.4997     0.1179   4.239  0.00218 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6663,    Adjusted R-squared:  0.6292 \nF-statistic: 17.97 on 1 and 9 DF,  p-value: 0.002176\n\nlm4 &lt;- lm(y4 ~ x4, data=anscombe)\nsummary(lm4)\n\n\nCall:\nlm(formula = y4 ~ x4, data = anscombe)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-1.751 -0.831  0.000  0.809  1.839 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)   \n(Intercept)   3.0017     1.1239   2.671  0.02559 * \nx4            0.4999     0.1178   4.243  0.00216 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.236 on 9 degrees of freedom\nMultiple R-squared:  0.6667,    Adjusted R-squared:  0.6297 \nF-statistic:    18 on 1 and 9 DF,  p-value: 0.002165\n\nplot(anscombe$x1,anscombe$y1)\nabline(coefficients(lm1))\n\n\n\nplot(anscombe$x2,anscombe$y2)\nabline(coefficients(lm2))\n\n\n\nplot(anscombe$x3,anscombe$y3)\nabline(coefficients(lm3))\n\n\n\nplot(anscombe$x4,anscombe$y4)\nabline(coefficients(lm4))\n\n\n\n## Fancy version (per help file)\n\nff &lt;- y ~ x\nmods &lt;- setNames(as.list(1:4), paste0(\"lm\", 1:4))\n\n# Plot using for loop\n\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  ## or   ff[[2]] &lt;- as.name(paste0(\"y\", i))\n  ##      ff[[3]] &lt;- as.name(paste0(\"x\", i))\n  mods[[i]] &lt;- lmi &lt;- lm(ff, data = anscombe)\n  print(anova(lmi))\n}\n\nAnalysis of Variance Table\n\nResponse: y1\n          Df Sum Sq Mean Sq F value  Pr(&gt;F)   \nx1         1 27.510 27.5100   17.99 0.00217 **\nResiduals  9 13.763  1.5292                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y2\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx2         1 27.500 27.5000  17.966 0.002179 **\nResiduals  9 13.776  1.5307                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y3\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx3         1 27.470 27.4700  17.972 0.002176 **\nResiduals  9 13.756  1.5285                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nAnalysis of Variance Table\n\nResponse: y4\n          Df Sum Sq Mean Sq F value   Pr(&gt;F)   \nx4         1 27.490 27.4900  18.003 0.002165 **\nResiduals  9 13.742  1.5269                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nsapply(mods, coef)  # Note the use of this function\n\n                  lm1      lm2       lm3       lm4\n(Intercept) 3.0000909 3.000909 3.0024545 3.0017273\nx1          0.5000909 0.500000 0.4997273 0.4999091\n\nlapply(mods, function(fm) coef(summary(fm)))\n\n$lm1\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0000909  1.1247468 2.667348 0.025734051\nx1          0.5000909  0.1179055 4.241455 0.002169629\n\n$lm2\n            Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.000909  1.1253024 2.666758 0.025758941\nx2          0.500000  0.1179637 4.238590 0.002178816\n\n$lm3\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0024545  1.1244812 2.670080 0.025619109\nx3          0.4997273  0.1178777 4.239372 0.002176305\n\n$lm4\n             Estimate Std. Error  t value    Pr(&gt;|t|)\n(Intercept) 3.0017273  1.1239211 2.670763 0.025590425\nx4          0.4999091  0.1178189 4.243028 0.002164602\n\n# Preparing for the plots\nop &lt;- par(mfrow = c(2, 2), mar = 0.1+c(4,4,1,1), oma =  c(0, 0, 2, 0))\n\n# Plot charts using for loop\nfor(i in 1:4) {\n  ff[2:3] &lt;- lapply(paste0(c(\"y\",\"x\"), i), as.name)\n  plot(ff, data = anscombe, col = \"blue\", pch = 21, bg = \"yellow\", cex = 1.2,\n       xlim = c(3, 19), ylim = c(3, 13))\n  abline(mods[[i]], col = \"red\")\n}\nmtext(\"Anscombe's 4 Regression data sets\", outer = TRUE, cex = 1.5)\n\n\n\npar(op)"
  },
  {
    "objectID": "assignment4.html",
    "href": "assignment4.html",
    "title": "assignment4",
    "section": "",
    "text": "# Assignment 4 Data Vis Hackathon \n\n# Tamnala - Dohyo Group \n# Sept 27, 2022\n\nrm(list=ls())\n\nlibrary(readr)\n\nWarning: package 'readr' was built under R version 4.2.1\n\nN<- read_csv(\"Sampling Example.csv\") \n\nRows: 1000 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (2): Name, Music Taste\ndbl (3): Age, Political Party - Liberal (0)/Conservative (1), Likeliness you...\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\nVoting <- N$`Likeliness you will vote`\nGenre <- N$`Music Taste`\nAge <- N$Age\nParty <- N$`Political Party - Liberal (0)/Conservative (1)`\n\nlibrary(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.2.1\n\nlibrary(tidyverse)\n\nWarning: package 'tidyverse' was built under R version 4.2.1\n\n\n── Attaching packages ─────────────────────────────────────── tidyverse 1.3.2 ──\n✔ tibble  3.1.8      ✔ dplyr   1.0.10\n✔ tidyr   1.2.1      ✔ stringr 1.4.1 \n✔ purrr   0.3.4      ✔ forcats 0.5.2 \n\n\nWarning: package 'tibble' was built under R version 4.2.1\n\n\nWarning: package 'tidyr' was built under R version 4.2.1\n\n\nWarning: package 'purrr' was built under R version 4.2.1\n\n\nWarning: package 'dplyr' was built under R version 4.2.1\n\n\nWarning: package 'stringr' was built under R version 4.2.1\n\n\nWarning: package 'forcats' was built under R version 4.2.1\n\n\n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\n\nlibrary(viridis)\n\nWarning: package 'viridis' was built under R version 4.2.1\n\n\nLoading required package: viridisLite\n\n\nWarning: package 'viridisLite' was built under R version 4.2.1\n\nlibrary(gridExtra)\n\nWarning: package 'gridExtra' was built under R version 4.2.1\n\n\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(cowplot)\n\nWarning: package 'cowplot' was built under R version 4.2.1\n\np1<- ggplot(N,\n            aes(x=`Voting`,\n            ))+\n  geom_bar(position='dodge') +\n  scale_colour_brewer(palette = \"Set1\") +\n  theme_bw() +\n  labs(x = \"On a Scale of 1-to-5 How Likely Are You to Vote?\",\n       y = \"count\",\n       title = \"Likeliness of Voting\",\n       caption = Sys.Date())\np1\n\n\n\n\nGraph A\nOn a scale of 1 to 5 where 1 indicates a strong disagreement to voting and 5 indicates a strong preference for voting, most respondents indicate their preference of being undecided to being not likely to vote, while those that indicate stronger preferences towards voting are fewer within the distribution.\n\n#2\n\np2<- ggplot(N,\n            aes(x=`Genre`,\n            ))+\n  geom_bar(position='dodge') +\n  scale_colour_brewer(palette = \"Set1\") +\n  theme_bw() +\n  labs(x = \"Preferred Genre?\",\n       y = \"count\",\n       title = \"Genre\",\n       caption = Sys.Date())\np2\n\n\n\n\nGraph B\nIn terms of music genre, Latin and Pop music are the most preferred, followed closely by R & B, and Classical, while Country and Other genres have low preferences.\n\np3<- ggplot(N,\n            aes(x=`Age`,\n            ))+\n  geom_bar(position='dodge') +\n  scale_colour_brewer(palette = \"Set1\") +\n  theme_bw() +\n  labs(x = \"How old are you?\",\n       y = \"count\",\n       title = \"Age\",\n       caption = Sys.Date())\np3\n\n\n\n\nGraph C\nThe age distribution indicates that most individuals are 55 years old, followed closely by individuals aged 31 years, 28 years, and 22 years respectively. The fewest individuals are aged 58 years, 26 years, and 24 years respectively. While other age groups are fairly distributed\n\nggdraw() +\n  draw_plot(p2, x = 0, y = .5, width = .5, height = .5) +\n  draw_plot(p3, x = .5, y = .5, width = .5, height = .5) +\n  draw_plot(p1, x = 0, y = 0, width = 1, height = 0.5) +\n  draw_plot_label(label = c(\"A\", \"B\", \"C\"), size = 8,\n                  x = c(0, 0.5, 0), y = c(1, 1, 0.5))"
  },
  {
    "objectID": "assignment5.html",
    "href": "assignment5.html",
    "title": "assignment5",
    "section": "",
    "text": "library(RColorBrewer)\n\n\nhpi <- read.csv(\"hpi_count.csv\")\n\ncolnames(hpi) <- c(\"hpi\", \"gdp\", \"continent\", \"year\")\n\nhpi <- hpi %>% mutate(year = as.character(year))\n\np1 <- ggplot(hpi,\n       aes(x = continent,\n           group = year,\n           fill = year))+\n  scale_fill_brewer(palette = \"Accent\") +\n  geom_bar(position='dodge') +\n  coord_flip() +\n  labs(title = \"Change of High Happiness Frequency by Continent\") +\n  theme_classic()\n\np1\n\n\n\n\n\np2 <- ggplot(hpi,\n             aes(x = year,\n                 group = continent,\n                 fill = continent))+\n  scale_fill_manual(breaks = c(\"Asia\", \"Europe\", \"S.Africa\"),\n                    values = c(\"pink\", \"yellow\", \"grey\")) +\n  geom_bar(position='dodge') +\n  labs(title = \"Continents with high happiness index by year\") +\n  theme_classic()\np2\n\n\n\n\n\np3 <- ggplot(hpi, aes(x = hpi, fill = continent)) +\n  geom_histogram(binwidth = 15, boundary = -7.5) +\n  coord_polar() +\n  scale_x_continuous(limits = c(20, 70)) +\n  scale_fill_brewer(palette = \"Set2\") +\n  xlab(\"Happy index\")+\n  labs(title = \"Happy index by Continent\")\n\np3\n\nWarning: Removed 6 rows containing missing values (geom_bar)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Tamnala Briggs-Megafu",
    "section": "",
    "text": "This is Tamnala Briggs-Megafu’s website. Data projects are hosted here.\npar (family=\"sans\")\nplot(iris, pch=20, cex=.75, col=\"steelblue\")\npar(family=“Arial”)\nAssignment #1\n1). QT Movie rental downloaded, and new project created.\nhttps://utdallas.yul1.qualtrics.com/survey-builder/SV_ewTQQWsSK3XjlMq/edit\n2). Analyze Survey\na) How is the survey structured – There is a page break after the first block, and the entire survey is made up of 18 blocks\nb) What is the questionnaire composed of – The questionnaire is composed of text graphic, multiple choice, and matrix table type questions.\nc) How are the questions ordered – The questionnaire starts with a recruitment letter followed by a page break. This is followed by 12 questions about customers’ perceptions about the services received and followed by 4 questions about customers’ demographics.\n3). Change the look & feel to include UT Dallas Header -\n4). Use skip logic in Q4, if answer is NO, skip to the question Q7 “Do you feel comfortable purchasing software over the internet? What is the difference between Skip Logic and Display Logic?\nSkip logic allows you to send the respondents to a future point in the survey based on how they answer a question while Display logic will display the selected question if certain criteria are met.\n5. Apply validation to every question - force response\n6. Insert page break to save respondents from scrolling down the screen\n7) Reorganize the survey by blocks – moved Q6 to be just after Q4\nii) Apply display logic to one of the questions – Display logic applied to Q11 to reflect if customer selected already own a DVD playerfrom Q3\niii) Try out the Carry Forward features\niv) How to make the survey mobile friendly?\n1)     Add page breaks to every question – page break added to every question to make the survey mobile friendly\n2)     Apply validation to every question – Validation (force response) added to every question.\n3)     Reconsider Matrix table?\nv) Add-on challenges\n1) Add default choices\nDefault choices can be used to feature answer choices within a question when respondents open a survey. This can be used when you want respondents to update previously collected data, such as updating their contact addresses or changes in their names if they get married for example. Also, respondents can use this feature to give suggestions on the survey.\nAssignment #2\nThe difference between the two programs comes down to intuition. If the purpose is to visualize data from a non social science research perspective GTrends is much better at doing this. However, if the goal is to both analyze and perform research the gtrends package is much better.\nIt would be interesting to perform an analysis importing the csv from google versus using the gtrends package.\nAssignment #3\n3a.: The Biden-Xi analysis allows for an understanding of what is going on within United States international relations and what revolves around the conversation. Terms like coronavirus and fentanyl stand out, because of the controversy surrounding COVID-19, as well as the flow of fentanyl into the United States that is in part due to the drug coming from the souther border being of Chinese origin. In addition to the aforementioned terms it is important to note the role of human rights and the tension between rising Chinese hegemony and the United States hegemonic position being challenged by it. Since the U.S. represents Western Liberal democratic order it is possible to see how human rights ties into the crises involving both Tibetans and Uyghers in China.\n3b.: Looking over time, it is important to note that at least within the plot there is very little change in the use of the term “american” and this is very similar to the the use of the word “people”. Across time, these terms do not look like they change across time. There are however a couple of anomalies within the plot - The Trump 2017 speach being one of these instances, where his use of the term “american” is greater than everybody else. An explanation for this may be due to his populist platform that appeases to his voter base. A similar thing is found with people in both Johnson and Nixon regarding people, but a qualitative analysis may be necessary to know why they used these terms to the extent they did. For analyses regarding ideology it would be interesting to do a similar analysis between the term “communist” and “socialist” - I attempted an analysis and was not able to get a plot for it.\nAssignment #5\nThere were issues in my ability to access this data. However, listed below is what it looked like. As you can tell there is an original ID and secret. Issues regarding third-party apps made it difficult to continue to the data analysis portion of the assignment.\n# = Autenthication = # # # yt_oauth(“207302960577-7uqd83qilb3586f0dj77p2h4hq06kjs0.apps.googleusercontent.com”,“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”, token = ““)\nyt_israelprotest = yt_search(term = “Israel protest”)"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "research",
    "section": "",
    "text": "About this site\n\n# Final Project\n## Final Paper\n&lt;iframe src=“https://drive.google.com/file/d/11eTnzgdzBV_P6gQjJ6kDUWflnTOMojYX/preview” width=”640” height=“480” allow=“autoplay”&gt; &lt;/iframe&gt;\n## Presentation Slides\n&lt;iframe src=“https://drive.google.com/file/d/11g-pJtcvaH4iLIfHxIjDWv91FCA8hAhI/preview” width=”640” height=“480” allow=“autoplay”&gt; &lt;/iframe&gt;"
  },
  {
    "objectID": "assignment6.html",
    "href": "assignment6.html",
    "title": "assignment6",
    "section": "",
    "text": "My Shiny App"
  },
  {
    "objectID": "index.html#workshop-scraping-webpages-with-r-rvest-package-prerequisites-chrome-browser-selector-gadget",
    "href": "index.html#workshop-scraping-webpages-with-r-rvest-package-prerequisites-chrome-browser-selector-gadget",
    "title": "Tamnala Briggs-Megafu",
    "section": "Workshop: Scraping webpages with R rvest package # Prerequisites: Chrome browser, Selector Gadget",
    "text": "Workshop: Scraping webpages with R rvest package # Prerequisites: Chrome browser, Selector Gadget\n#install.packages(“tidyverse”) library(tidyverse) #install.packages(“rvest”) library(rvest)\nurl &lt;- ‘https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves’ #Reading the HTML code from the Wiki website wikiforreserve &lt;- read_html(url) class(wikiforreserve)"
  },
  {
    "objectID": "index.html#get-the-xpath-data-using-inspect-element-feature-in-safari-chrome-or-firefox",
    "href": "index.html#get-the-xpath-data-using-inspect-element-feature-in-safari-chrome-or-firefox",
    "title": "Tamnala Briggs-Megafu",
    "section": "Get the XPath data using Inspect element feature in Safari, Chrome or Firefox",
    "text": "Get the XPath data using Inspect element feature in Safari, Chrome or Firefox\n## At Inspect tab, look for\n\ntag. Leave the table close ## Right click the table and Copy XPath, paste at html_nodes(xpath =)foreignreserve &lt;- wikiforreserve %&gt;% html_nodes(xpath=‘//*[@id=\"mw-content-text\"]/div/table[1]’) %&gt;% html_table() class(foreignreserve) fores = foreignreserve[[1]]names(fores) &lt;- c(“Rank”, “Country”, “Forexres”, “Date”, “Change”, “Sources”) colnames(fores)head(fores$Country, n=10)\nClean up variables\n\nWhat type is Rank?\n\nHow about Date?\n\nRemove trailing notes in Date variable\nlibrary(stringr) fores\\(newdate = str_split_fixed(fores\\)Date, “\\[”, n = 2)[, 1]\nwrite.csv(fores, “fores.csv”, row.names = FALSE) ~~~\nImage of csv “Fores”\nScript rvest02\n\nWorkshop: Scraping webpages with R rvest package # Prerequisites: Chrome browser, Selector Gadget\n\n\ninstall.packages(“tidyverse”)\nlibrary(tidyverse) # install.packages(“rvest”) url1 = “https://www.imdb.com/search/title/?release_date=2022-01-01,2023-01-01” imdb2022 &lt;- read_html(url1) rank_data_html &lt;- html_nodes(imdb2022,‘.text-primary’) rank_data &lt;- as.numeric(html_text(rank_data_html)) head(rank_data, n = 10) title_data_html &lt;- html_nodes(imdb2022,‘.lister-item-header a’) title_data &lt;- html_text(title_data_html)\nhead(title_data, n =20)\nThe difficulty of this is that I am unsure as to how to make this visual.\nAssignment #7\nCode for first part of govdata with output\n## Scraping Government data\n## Website: GovInfo (https://www.govinfo.gov/app/search/)\n## Prerequisite: Download from website the list of files to be downloaded\n## Designed for background job\n\n# Start with a clean plate and lean loading to save memory\n \ngc(reset=T)\nrm(list = ls())\n\n#install.packages(c(\"purrr\", \"magrittr\")\ninstall.packages(\"purr\")\ninstall.packages(\"magrittr\")\nlibrary(purrr)\nlibrary(magrittr)\n\n## Set path for reading the listing and home directory\n## For Windows, use \"c:\\\\directory\\\\subdirectory\"\nlibrary(readr)\n\n\ngovfiles= read_csv(\"govinfo-search-results-2023-12-05T16_37_50.csv\")\nView(govfiles)\n\n\n# Directory to save the pdf's\nsave_dir &lt;- \"C:\\\\epps6302\\\\pdf\"\n\n# Function to download pdfs\ndownload_govfiles_pdf &lt;- function(url, id) {\n  tryCatch({\n    destfile &lt;- paste0(save_dir, \"govfiles_\", id, \".pdf\")\n    download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n    Sys.sleep(runif(1, 1, 3))  # Important: random sleep between 1 and 3 seconds to avoid suspicion of \"hacking\" the server\n    return(paste(\"Successfully downloaded:\", url))\n  },\n  error = function(e) {\n    return(paste(\"Failed to download:\", url))\n  })\n}\n\n# Download files, potentially in parallel for speed\n# Simple timer, can use package like tictoc\nstart.time &lt;- Sys.time()\nmessage(\"Starting downloads\")\nresults &lt;- 1:length(pdf_govfiles_url) %&gt;% \n  purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\nmessage(\"Finished downloads\")\nend.time &lt;- Sys.time()\ntime.taken &lt;- end.time - start.time\ntime.taken\n\n# Print results\nprint(results)\n118th Congress Congressional Hearings in Committee on Foreign Affairs csv\nA main issue with the parallel R file is the issue of cores and difficulty with overcoming this and constantly receiving and error. Code Below and output as well\n\nParallelization\n\n\nPrerequisite: Multiple core on CPU\n\n\nLoad the parallel package\nlibrary(parallel) library(pdftools)\n\nCreate a function to be applied in parallelizing later jobs\nread_pdf_to_text &lt;- function(uri) { text &lt;- pdftools::pdf_text(uri) return(text) } # For mac\npdf_texts &lt;- mclapply(pdfpath, read_pdf_to_text, mc.cores = num_cores) toc()\n\nFor all platforms\n\n\nLoad the parallel package\nlibrary(parallel) library(pdftools) # Get the number of cores available on your machine num_cores &lt;- detectCores()\n\nInitialize a cluster with the number of available cores\ncl &lt;- makeCluster(num_cores)\n\nLoad libraries and functions in each cluster\nclusterEvalQ(cl, library(pdftools))\n\nDefine the function to be parallelized after making the clusters\nread_pdf_to_text &lt;- function(uri) { text &lt;- pdftools::pdf_text(uri) return(text) }\n\nExport any libraries or objects that will be used within the parallel code\n\nPerform the parallel computation\ntic() pdf_texts &lt;- parLapply(cfa_ch, read_pdf_to_text, mc.cores = num_cores) toc()\n\nDon’t forget to stop the cluster\nstopCluster(cl)\nAssignment #8\nA couple of observations regarding setting up the census data key. First, it is a relative simple process. However, the second, it is much faster if one does not use an .edu email.\nMy key is: 311dcd5892541c0eaa28747e5024f1e308c5xxxx\nBelow are 2019 and 2009 codes and images. Although the assignment asks for 2020, this is not available.\n\nGet a list of American Community Survey (ACS) 2019 variables\nacs19 = tidycensus::load_variables(2019, “acs5”, cache = TRUE) acs19_Profile = load_variables(2019 , “acs5/profile”, cache = TRUE) us_median_age19 &lt;- get_acs( geography = “state”, variables = “B01002_001”, year = 2019, survey = “acs1”, geometry = TRUE, resolution = “20m” ) %&gt;% shift_geometry()\nplot(us_median_age19$geometry) ggplot(data = us_median_age19, aes(fill = estimate)) + geom_sf(col=“white”) + # Why color is white? theme_bw() + scale_fill_distiller(palette = “PuBuGn”, # Try other palette? direction = 1) + labs(title = ” Median Age by State, 2019”, caption = “Data source: 2019 1-year ACS, US Census Bureau”, fill = ““, family=”Palatino”) + theme(legend.position=c(.08,.6), legend.direction=“vertical”) + theme(text = element_text(family = “Palatino”), plot.title = element_text(hjust = 0.5))\n\nGet a list of American Community Survey (ACS) 2009 variables\nacs09 = tidycensus::load_variables(2009, “acs5”, cache = TRUE) acs09_Profile = load_variables(2009 , “acs5/profile”, cache = TRUE) us_median_age09 &lt;- get_acs( geography = “state”, variables = “B01002_001”, year = 2009, survey = “acs1”, geometry = TRUE, resolution = “20m” ) %&gt;% shift_geometry()\nplot(us_median_age09$geometry) ggplot(data = us_median_age09, aes(fill = estimate)) + geom_sf(col=“green”) + # Why color is white? theme_bw() + scale_fill_distiller(palette = “PuBuGn”, # Try other palette? direction = 1) + labs(title = ” Median Age by State, 2009”, caption = “Data source: 2009 1-year ACS, US Census Bureau”, fill = ““, family=”Palatino”) + theme(legend.position=c(.08,.6), legend.direction=“vertical”) + theme(text = element_text(family = “Palatino”), plot.title = element_text(hjust = 0.5))\nThere are states where we can observe age go up such as Nevada, Arizona, and New Mexico. We see a very similar trend in Puerto Rico. This may be due to the retirees heading to these locations."
  },
  {
    "objectID": "index.html#clean-up-variables",
    "href": "index.html#clean-up-variables",
    "title": "Tamnala Briggs-Megafu",
    "section": "Clean up variables",
    "text": "Clean up variables"
  },
  {
    "objectID": "index.html#what-type-is-rank",
    "href": "index.html#what-type-is-rank",
    "title": "Tamnala Briggs-Megafu",
    "section": "What type is Rank?",
    "text": "What type is Rank?"
  },
  {
    "objectID": "index.html#how-about-date",
    "href": "index.html#how-about-date",
    "title": "Tamnala Briggs-Megafu",
    "section": "How about Date?",
    "text": "How about Date?"
  },
  {
    "objectID": "index.html#workshop-scraping-webpages-with-r-rvest-package-prerequisites-chrome-browser-selector-gadget-1",
    "href": "index.html#workshop-scraping-webpages-with-r-rvest-package-prerequisites-chrome-browser-selector-gadget-1",
    "title": "Tamnala Briggs-Megafu",
    "section": "Workshop: Scraping webpages with R rvest package # Prerequisites: Chrome browser, Selector Gadget",
    "text": "Workshop: Scraping webpages with R rvest package # Prerequisites: Chrome browser, Selector Gadget"
  },
  {
    "objectID": "index.html#parallelization",
    "href": "index.html#parallelization",
    "title": "Tamnala Briggs-Megafu",
    "section": "Parallelization",
    "text": "Parallelization"
  },
  {
    "objectID": "index.html#prerequisite-multiple-core-on-cpu",
    "href": "index.html#prerequisite-multiple-core-on-cpu",
    "title": "Tamnala Briggs-Megafu",
    "section": "Prerequisite: Multiple core on CPU",
    "text": "Prerequisite: Multiple core on CPU"
  },
  {
    "objectID": "index.html#for-all-platforms",
    "href": "index.html#for-all-platforms",
    "title": "Tamnala Briggs-Megafu",
    "section": "For all platforms",
    "text": "For all platforms"
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments",
    "section": "",
    "text": "Assignment #1\nANALYZE SURVEY\na-c: The questions on the survey are relatively simple and non complex. Given that they relate to movie rentals they are particular to this topic. They begin with multiple choice/ranking questions and then are set up with multiple choice questions, but for the most part questions that are strictly qualitative.\nPersonally, my only criticism involves design. The questions seem long and from a survey design perspective not intuitive for mobile users.\nd,3-7: The survey is fairly intuitive and does not need editing, it does not deviate much from pre-existing surveys. Adding the UTD logo personalizes it to the university which makes it useful for UTD users.\nFURTHER ASSIGNMENT\n1-4: Inserting this block questions allows for the introduction of multiple questions at a time, assuming these blocks are preexisting they create an easier survey experience.\n\n\nAssignment #2\nThe difference between the two programs comes down to intuition. If the purpose is to visualize data from a non social science research perspective GTrends is much better at doing this. However, if the goal is to both analyze and perform research the gtrends package is much better.\nIt would be interesting to perform an analysis importing the csv from google versus using the gtrends package.\n\n\nAssignment #3\n3a.: The Biden-Xi analysis allows for an understanding of what is going on within United States international relations and what revolves around the conversation. Terms like coronavirus and fentanyl stand out, because of the controversy surrounding COVID-19, as well as the flow of fentanyl into the United States that is in part due to the drug coming from the souther border being of Chinese origin. In addition to the aforementioned terms it is important to note the role of human rights and the tension between rising Chinese hegemony and the United States hegemonic position being challenged by it. Since the U.S. represents Western Liberal democratic order it is possible to see how human rights ties into the crises involving both Tibetans and Uyghers in China.\n\n3b.: Looking over time, it is important to note that at least within the plot there is very little change in the use of the term “american” and this is very similar to the the use of the word “people”. Across time, these terms do not look like they change across time. There are however a couple of anomalies within the plot - The Trump 2017 speach being one of these instances, where his use of the term “american” is greater than everybody else. An explanation for this may be due to his populist platform that appeases to his voter base. A similar thing is found with people in both Johnson and Nixon regarding people, but a qualitative analysis may be necessary to know why they used these terms to the extent they did. For analyses regarding ideology it would be interesting to do a similar analysis between the term “communist” and “socialist” - I attempted an analysis and was not able to get a plot for it.\n\n\n\nAssignment #5\n\nThere were issues in my ability to access this data. However, listed below is what it looked like. As you can tell there is an original ID and secret. Issues regarding third-party apps made it difficult to continue to the data analysis portion of the assignment.\n\n\\# = Autenthication = \\# \\# \\# yt_oauth(\"207302960577-7uqd83qilb3586f0dj77p2h4hq06kjs0.apps.googleusercontent.com\",\"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\", token = \"\")\n\nyt_israelprotest = yt_search(term = \"Israel protest\")\n\n# List of categories (region filter: US)\n\nvideocat_us= list_videocats(c(region_code = \"us\")) \\# = Download and prepare data = \\# mostpop = list_videos()\n\nmostpop_us = list_videos(video_category_id = \"25\", region_code = \"US\", max_results = 10)\n\n# Find the channel ID in the source page\n\n# Alternatively, from get_video_details\n\n# = Channel stats =\n\nnbcnews_stat = get_channel_stats(\"UCeY0bbntWzzVIaj2z3QigXg\") nbcnews_detail = get_video_details(video_id = \"to0YqKKRIWY\")\n\n# = Videos =\n\ncurl::curl_version() httr::set_config(httr::config(http_version = 0)) \\# Fix curl issue\n\nnbc_videos1 = yt_search(term=\"\", type=\"video\", channel_id = \"UCeY0bbntWzzVIaj2z3QigXg\") nbc_videos = nbc_videos1 %\\&gt;% mutate(date = as.Date(publishedAt)) %\\&gt;% filter(date \\&gt; \"2022-11-27\") %\\&gt;% arrange(date) samplecomment = get_comment_threads(c(video_id = \"to0YqKKRIWY\"), max_results = 600) samplecomment2 = get_all_comments(c(video_id = \"to0YqKKRIWY\"), max_results = 600) \\# = Comments, may take a long time \\# nbc_comments = lapply(as.character(nbc_videos1\\$video_id), function(x){ get_comment_threads(c(video_id = x), max_results = 101) }) \na. An analysis of CNN would be the following. Keeping all things the same, except the channel ID.\n# = Autenthication = #\n# \n# \nyt_oauth(\"207302960577-7uqd83qilb3586f0dj77p2h4hq06kjs0.apps.googleusercontent.com\",\"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\", token = \"\")\n\nyt_israelprotest = yt_search(term = \"Israel protest\")\n\n# List of categories (region filter: US)\nvideocat_us= list_videocats(c(region_code = \"us\"))\n# = Download and prepare data = #\nmostpop = list_videos()\n\nmostpop_us = list_videos(video_category_id = \"25\",   region_code = \"US\", max_results = 10)\n\n\n# Find the channel ID in the source page\n# Alternatively, from get_video_details\n# = Channel stats = #\n\ncnn_stat = get_channel_stats(\"UCeY0bbntWzzVIaj2z3QigXg\")\ncnn_detail = get_video_details(video_id = \"to0YqKKRIWY\")\n\n\n\n# = Videos = #\ncurl::curl_version()\nhttr::set_config(httr::config(http_version = 0)) # Fix curl issue\n\ncnn_videos1 = yt_search(term=\"\", type=\"video\", channel_id = \"UCeY0bbntWzzVIaj2z3QigXg\")\ncnn_videos = nbc_videos1 %&gt;%\n  mutate(date = as.Date(publishedAt)) %&gt;%\n  filter(date &gt; \"2022-11-27\") %&gt;%\n  arrange(date)\nsamplecomment = get_comment_threads(c(video_id = \"to0YqKKRIWY\"), max_results = 600)\nsamplecomment2 = get_all_comments(c(video_id = \"to0YqKKRIWY\"), max_results = 600)\n# = Comments, may take a long time #\ncnn_comments = lapply(as.character(nbc_videos1$video_id), function(x){\n  get_comment_threads(c(video_id = x), max_results = 101)\n})\n\nI can potentially use quanteda, but given the difficulty accessing third party apps it does not seem possible. It would be interesting to see a data scientist analyzing their own child do this.\n\n\n\nAssignment #6\n\nThese are the wordclouds from the rscript textmining and the code used\n\n \\# Data Method: Text mining \\# File: textmining1.R \\# Theme: Download text data from web and create wordcloud\n\n    # Install the easypackages package\n\n    install.packages(\"easypackages\") library(easypackages)\n\n    # Download text data from website\n\n    mlkLocation \\&lt;-URLencode(\"http://www.analytictech.com/mb021/mlk.htm\")\n\n    # use htmlTreeParse function to read and parse paragraphs\n\n    doc.html\\&lt;- htmlTreeParse(mlkLocation, useInternal=TRUE) mlk \\&lt;- unlist(xpathApply(doc.html, '//p', xmlValue)) mlk head(mlk, 3)\n\n    # Vectorize mlk\n\n    words.vec \\&lt;- VectorSource(mlk)\n\n    # Check the class of words.vec\n\n    class(words.vec)\n\n    # Create Corpus object for preprocessing\n\n    words.corpus \\&lt;- Corpus(words.vec) inspect(words.corpus)\n\n    # Turn all words to lower case\n\n    words.corpus \\&lt;- tm_map(words.corpus, content_transformer(tolower))\n\n    # Remove punctuations, numbers\n\n    words.corpus \\&lt;- tm_map(words.corpus, removePunctuation) words.corpus \\&lt;- tm_map(words.corpus, removeNumbers)\n\n    # How about stopwords, then uniform bag of words created\n\n    words.corpus \\&lt;- tm_map(words.corpus, removeWords, stopwords(\"english\"))\n\n    # Create Term Document Matrix\n\n    tdm \\&lt;- TermDocumentMatrix(words.corpus) inspect(tdm)\n\n    m \\&lt;- as.matrix(tdm) wordCounts \\&lt;- rowSums(m) wordCounts \\&lt;- sort(wordCounts, decreasing=TRUE) head(wordCounts)\n\n    # Create Wordcloud\n\n    cloudFrame\\&lt;-data.frame(word=names(wordCounts),freq=wordCounts)\n\n    set.seed(1234) wordcloud(cloudFrame$word,cloudFrame$freq) wordcloud(names(wordCounts),wordCounts, min.freq=3,random.order=FALSE, max.words=500,scale=c(3,.5), rot.per=0.35,colors=brewer.pal(8,\"Dark2\"))\n    \n\n\nThe following is from the Winston Churchill speech, code and images\n \\# Download text data from website wcLocation \\&lt;-URLencode(\"http://www.historyplace.com/speeches/churchill-hour.htm\")\n\n# use htmlTreeParse function to read and parse paragraphs\n\ndoc.html\\&lt;- htmlTreeParse(wcLocation, useInternal=TRUE) wc \\&lt;- unlist(xpathApply(doc.html, '//p', xmlValue)) wc head(wc, 3)\n\n# Vectorize wc\n\nwords.vec \\&lt;- VectorSource(wc)\n\n# Check the class of words.vec\n\nclass(words.vec)\n\n# Create Corpus object for preprocessing\n\nwords.corpus \\&lt;- Corpus(words.vec) inspect(words.corpus)\n\n# Turn all words to lower case\n\nwords.corpus \\&lt;- tm_map(words.corpus, content_transformer(tolower))\n\n# Remove punctuations, numbers\n\nwords.corpus \\&lt;- tm_map(words.corpus, removePunctuation) words.corpus \\&lt;- tm_map(words.corpus, removeNumbers)\n\n# How about stopwords, then uniform bag of words created\n\nwords.corpus \\&lt;- tm_map(words.corpus, removeWords, stopwords(\"english\"))\n\n# Create Term Document Matrix\n\ntdm \\&lt;- TermDocumentMatrix(words.corpus) inspect(tdm)\n\nm \\&lt;- as.matrix(tdm) wordCounts \\&lt;- rowSums(m) wordCounts \\&lt;- sort(wordCounts, decreasing=TRUE) head(wordCounts)\n\n# Create Wordcloud\n\ncloudFrame\\&lt;-data.frame(word=names(wordCounts),freq=wordCounts)\n\nset.seed(1234) wordcloud(cloudFrame$word,cloudFrame$freq) wordcloud(names(wordCounts),wordCounts, min.freq=3,random.order=FALSE, max.words=500,scale=c(3,.5), rot.per=0.35,colors=brewer.pal(8,\"Dark2\")) \\~\\~\\~\n\n\nCode for running rvest01\n## Workshop: Scraping webpages with R rvest package \\# Prerequisites: Chrome browser, Selector Gadget\n\n#install.packages(\"tidyverse\") library(tidyverse) #install.packages(\"rvest\") library(rvest)\n\nurl \\&lt;- 'https://en.wikipedia.org/wiki/List_of_countries_by_foreign-exchange_reserves' #Reading the HTML code from the Wiki website wikiforreserve \\&lt;- read_html(url) class(wikiforreserve)\n\n## Get the XPath data using Inspect element feature in Safari, Chrome or Firefox\n\n\\## At Inspect tab, look for\n\n&lt;table class=....&gt;\n\ntag. Leave the table close \\## Right click the table and Copy XPath, paste at html_nodes(xpath =)\n\nforeignreserve \\&lt;- wikiforreserve %\\&gt;% html_nodes(xpath='//\\*[@id=\"mw-content-text\"]/div/table\\[1\\]') %\\&gt;% html_table() class(foreignreserve) fores = foreignreserve\\[\\[1\\]\\]\n\nnames(fores) \\&lt;- c(\"Rank\", \"Country\", \"Forexres\", \"Date\", \"Change\", \"Sources\") colnames(fores)\n\nhead(fores\\$Country, n=10)\n\n## Clean up variables\n\n## What type is Rank?\n\n## How about Date?\n\n# Remove trailing notes in Date variable\n\nlibrary(stringr) fores$newdate = str_split_fixed(fores$Date, \"\\\\\\[\", n = 2)\\[, 1\\]\n\nwrite.csv(fores, \"fores.csv\", row.names = FALSE) \\~\\~\\~\nImage of csv “Fores”\n\nScript rvest02\n## Workshop: Scraping webpages with R rvest package \\# Prerequisites: Chrome browser, Selector Gadget\n\n# install.packages(\"tidyverse\")\n\nlibrary(tidyverse) \\# install.packages(\"rvest\") url1 = \"https://www.imdb.com/search/title/?release_date=2022-01-01,2023-01-01\" imdb2022 \\&lt;- read_html(url1) rank_data_html \\&lt;- html_nodes(imdb2022,'.text-primary') rank_data \\&lt;- as.numeric(html_text(rank_data_html)) head(rank_data, n = 10) title_data_html \\&lt;- html_nodes(imdb2022,'.lister-item-header a') title_data \\&lt;- html_text(title_data_html)\n\nhead(title_data, n =20)\nThe difficulty of this is that I am unsure as to how to make this visual.\n\n\nAssignment #7\n\nCode for first part of govdata with output\n\n    ## Scraping Government data\n    ## Website: GovInfo (https://www.govinfo.gov/app/search/)\n    ## Prerequisite: Download from website the list of files to be downloaded\n    ## Designed for background job\n\n    # Start with a clean plate and lean loading to save memory\n     \n    gc(reset=T)\n    rm(list = ls())\n\n    #install.packages(c(\"purrr\", \"magrittr\")\n    install.packages(\"purr\")\n    install.packages(\"magrittr\")\n    library(purrr)\n    library(magrittr)\n\n    ## Set path for reading the listing and home directory\n    ## For Windows, use \"c:\\\\directory\\\\subdirectory\"\n    library(readr)\n\n\n    govfiles= read_csv(\"govinfo-search-results-2023-12-05T16_37_50.csv\")\n    View(govfiles)\n\n\n    # Directory to save the pdf's\n    save_dir &lt;- \"C:\\\\epps6302\\\\pdf\"\n\n    # Function to download pdfs\n    download_govfiles_pdf &lt;- function(url, id) {\n      tryCatch({\n        destfile &lt;- paste0(save_dir, \"govfiles_\", id, \".pdf\")\n        download.file(url, destfile = destfile, mode = \"wb\") # Binary files\n        Sys.sleep(runif(1, 1, 3))  # Important: random sleep between 1 and 3 seconds to avoid suspicion of \"hacking\" the server\n        return(paste(\"Successfully downloaded:\", url))\n      },\n      error = function(e) {\n        return(paste(\"Failed to download:\", url))\n      })\n    }\n\n    # Download files, potentially in parallel for speed\n    # Simple timer, can use package like tictoc\n    start.time &lt;- Sys.time()\n    message(\"Starting downloads\")\n    results &lt;- 1:length(pdf_govfiles_url) %&gt;% \n      purrr::map_chr(~ download_govfiles_pdf(pdf_govfiles_url[.], pdf_govfiles_id[.]))\n    message(\"Finished downloads\")\n    end.time &lt;- Sys.time()\n    time.taken &lt;- end.time - start.time\n    time.taken\n\n    # Print results\n    print(results)\n\n118th Congress Congressional Hearings in Committee on Foreign Affairs csv\n\nA main issue with the parallel R file is the issue of cores and difficulty with overcoming this and constantly receiving and error. Code Below and output as well\n## Parallelization\n## Prerequisite: Multiple core on CPU\n\n# Load the parallel package\nlibrary(parallel)\nlibrary(pdftools)\n\n# Create a function to be applied in parallelizing later jobs\nread_pdf_to_text &lt;- function(uri) {\n  text &lt;- pdftools::pdf_text(uri)\n  return(text)\n}\n# For mac\n\npdf_texts &lt;- mclapply(pdfpath, read_pdf_to_text, mc.cores = num_cores)\ntoc()\n\n## For all platforms\n# Load the parallel package\nlibrary(parallel)\nlibrary(pdftools)\n# Get the number of cores available on your machine\nnum_cores &lt;- detectCores()\n\n# Initialize a cluster with the number of available cores\ncl &lt;- makeCluster(num_cores)\n\n# Load libraries and functions in each cluster\nclusterEvalQ(cl, library(pdftools))\n\n# Define the function to be parallelized after making the clusters\nread_pdf_to_text &lt;- function(uri) {\n  text &lt;- pdftools::pdf_text(uri)\n  return(text)\n}\n\n# Export any libraries or objects that will be used within the parallel code\n\n# Perform the parallel computation\ntic()\npdf_texts &lt;- parLapply(cfa_ch, read_pdf_to_text, mc.cores = num_cores)\ntoc()\n\n# Don't forget to stop the cluster\nstopCluster(cl)\n\n\n\nAssignment #8\n\nA couple of observations regarding setting up the census data key. First, it is a relative simple process. However, the second, it is much faster if one does not use an .edu email.\n\nMy key is: 311dcd5892541c0eaa28747e5024f1e308c5xxxx\n\nBelow are 2019 and 2009 codes and images. Although the assignment asks for 2020, this is not available.\n\n\n# Get a list of American Community Survey (ACS) 2019 variables\nacs19 = tidycensus::load_variables(2019, \"acs5\", cache = TRUE)\nacs19_Profile = load_variables(2019 , \"acs5/profile\", cache = TRUE)\nus_median_age19 &lt;- get_acs(\n  geography = \"state\",\n  variables = \"B01002_001\",\n  year = 2019,\n  survey = \"acs1\",\n  geometry = TRUE,\n  resolution = \"20m\"\n) %&gt;%\n  shift_geometry()\n\nplot(us_median_age19$geometry)\nggplot(data = us_median_age19, aes(fill = estimate)) + \n  geom_sf(col=\"white\") +  # Why color is white?\n  theme_bw() +\n  scale_fill_distiller(palette = \"PuBuGn\",  # Try other palette?\n                       direction = 1) + \n  labs(title = \"  Median Age by State, 2019\",\n       caption = \"Data source: 2019 1-year ACS, US Census Bureau\",\n       fill = \"\", family=\"Palatino\") +\n  theme(legend.position=c(.08,.6), legend.direction=\"vertical\") +\n  theme(text = element_text(family = \"Palatino\"), plot.title = element_text(hjust = 0.5))\n\n# Get a list of American Community Survey (ACS) 2009 variables\n\nacs09 = tidycensus::load_variables(2009, \"acs5\", cache = TRUE)\nacs09_Profile = load_variables(2009 , \"acs5/profile\", cache = TRUE)\nus_median_age09 &lt;- get_acs(\n  geography = \"state\",\n  variables = \"B01002_001\",\n  year = 2009,\n  survey = \"acs1\",\n  geometry = TRUE,\n  resolution = \"20m\"\n) %&gt;%\n  shift_geometry()\n\nplot(us_median_age09$geometry)\nggplot(data = us_median_age09, aes(fill = estimate)) + \n  geom_sf(col=\"green\") +  # Why color is white?\n  theme_bw() +\n  scale_fill_distiller(palette = \"PuBuGn\",  # Try other palette?\n                       direction = 1) + \n  labs(title = \"  Median Age by State, 2009\",\n       caption = \"Data source: 2009 1-year ACS, US Census Bureau\",\n       fill = \"\", family=\"Palatino\") +\n  theme(legend.position=c(.08,.6), legend.direction=\"vertical\") +\n  theme(text = element_text(family = \"Palatino\"), plot.title = element_text(hjust = 0.5))\n\n\n\nThere are states where we can observe age go up such as Nevada, Arizona, and New Mexico. We see a very similar trend in Puerto Rico. This may be due to the retirees heading to these locations."
  }
]